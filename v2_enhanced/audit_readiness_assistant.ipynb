{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Audit Readiness Assistant\n",
    "\n",
    "**Capstone Project by: Abdulla Ahmed Alaydaroos**\n",
    "\n",
    "---\n",
    "\n",
    "## What This System Does\n",
    "\n",
    "This notebook demonstrates an **AI-powered audit readiness assistant** designed to help regulatory authorities assess whether organizations are ready for formal audits.\n",
    "\n",
    "The system helps auditors by:\n",
    "- Accepting **organizational information** and **compliance indicators**\n",
    "- Reviewing inputs against **regulatory and reporting requirements**\n",
    "- Identifying **potential compliance gaps** and **missing documentation**\n",
    "- Highlighting **high-risk areas** that need closer attention\n",
    "\n",
    "This tool acts as a **decision-support system** - it does not replace professional judgment, but helps auditors focus their efforts on the most important areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "Regulatory authorities like the Abu Dhabi Accountability Authority (ADAA) must ensure that organizations comply with:\n",
    "- Accounting standards (IFRS, local GAAP)\n",
    "- Reporting requirements\n",
    "- Internal control frameworks\n",
    "\n",
    "**Current challenges:**\n",
    "- Audit readiness assessments rely on **manual document reviews**\n",
    "- Auditors must check if records are maintained, disclosures are complete, and controls are in place\n",
    "- This process is **time-consuming** and can vary based on reviewer experience\n",
    "- Early warning signs of non-compliance may be **missed**\n",
    "\n",
    "**Solution:**\n",
    "An intelligent assistant that reviews audit inputs, identifies compliance gaps, and highlights areas requiring attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## How to Use This Notebook\n\n**Step 1:** Run the setup cells to install dependencies and configure the system\n\n**Step 2:** The demo documents will be created automatically (or upload your own)\n\n**Step 3:** Use the interactive interface to:\n1. Enter organization details\n2. Complete the compliance self-assessment checklist\n3. Add any previous audit findings\n4. Generate the audit readiness report\n\n**Step 4:** Review the gap analysis, recurring patterns, and compliance heat map\n\n**Step 5:** Download the PDF report and evidence checklist for audit preparation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install Dependencies\n# This cell installs all required Python packages\n\n!pip -q install --upgrade \\\n  openai==1.66.3 \\\n  langchain>=1.0.0 \\\n  langchain-core>=1.0.0 \\\n  langchain-openai>=0.3.0 \\\n  langchain-community>=0.3.0 \\\n  langgraph>=0.2.0 \\\n  tavily-python \\\n  faiss-cpu \\\n  sentence-transformers \\\n  pypdf \\\n  python-docx \\\n  gradio \\\n  pandas \\\n  reportlab \\\n  matplotlib\n\nprint(\"All packages installed successfully.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Configuration\n",
    "\n",
    "The system uses an OpenAI-compatible API for the language model.\n",
    "\n",
    "**Required Secrets (add via Colab Secrets sidebar):**\n",
    "- `OPEN_AI_API` - Your API key (required)\n",
    "- `TAVILY_API_KEY` - For web search capabilities (optional but recommended)\n",
    "\n",
    "Credentials are loaded securely from Colab Secrets and are never hardcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load API Credentials\n# This cell loads your API keys from Colab Secrets\n\nfrom google.colab import userdata\n\n# OpenAI-compatible API\nAPI_KEY = userdata.get(\"OPEN_AI_API\")\nassert API_KEY, \"Missing Colab Secret: OPEN_AI_API - Please add it in the Secrets sidebar\"\nBASE_URL = \"https://aibe.mygreatlearning.com/openai/v1\"\nprint(\"API key loaded successfully.\")\nprint(f\"Using endpoint: {BASE_URL}\")\n\n# Tavily API for web search (optional)\ntry:\n    TAVILY_API_KEY = userdata.get(\"TAVILY_API_KEY\")\n    if TAVILY_API_KEY:\n        print(\"Tavily API key loaded - web search is enabled\")\n    else:\n        TAVILY_API_KEY = None\n        print(\"Tavily API key not set - web search is disabled\")\nexcept Exception:\n    TAVILY_API_KEY = None\n    print(\"Tavily API key not found - web search is disabled\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core Imports\n# This cell imports all necessary Python libraries\n\nimport os\nimport io\nimport re\nimport json\nimport traceback\nimport uuid\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom dataclasses import dataclass, field, asdict\nfrom datetime import date, datetime\nfrom enum import Enum\nfrom pathlib import Path\n\nfrom pypdf import PdfReader\nimport docx\nimport pandas as pd\n\nprint(\"Core imports completed.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Architecture\n",
    "\n",
    "The audit readiness assistant follows a structured workflow:\n",
    "\n",
    "```\n",
    "WORKFLOW DIAGRAM:\n",
    "\n",
    "┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n",
    "│  ENTITY INTAKE  │────▶│ STANDARDS LOAD  │────▶│  GAP ANALYSIS   │\n",
    "│                 │     │                 │     │                 │\n",
    "│ - Org details   │     │ - IFRS rules    │     │ - Compare       │\n",
    "│ - Sector/size   │     │ - ADAA guides   │     │ - Find gaps     │\n",
    "│ - Framework     │     │ - Controls      │     │ - Score risk    │\n",
    "└─────────────────┘     └─────────────────┘     └─────────────────┘\n",
    "         │                                               │\n",
    "         ▼                                               ▼\n",
    "┌─────────────────┐                             ┌─────────────────┐\n",
    "│ COMPLIANCE INFO │                             │ READINESS REPORT│\n",
    "│                 │                             │                 │\n",
    "│ - Self-assess   │                             │ - Risk summary  │\n",
    "│ - Prior findings│                             │ - Gap details   │\n",
    "│ - Evidence      │                             │ - Actions needed│\n",
    "└─────────────────┘                             └─────────────────┘\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "1. **Entity Intake** - Collects organization profile and context\n",
    "2. **Compliance Assessment** - Self-assessment checklist and prior findings\n",
    "3. **Standards Retrieval** - Loads relevant regulations from knowledge base\n",
    "4. **Gap Analysis** - Compares inputs against requirements\n",
    "5. **Risk Scoring** - Prioritizes gaps by severity\n",
    "6. **Readiness Report** - Generates actionable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data Structures\n# This cell defines the core data structures used throughout the system\n\n# Entity types that can be assessed\nclass EntityType(str, Enum):\n    GOVERNMENT = \"Government Entity\"\n    SEMI_GOVERNMENT = \"Semi-Government Entity\"\n    PRIVATE = \"Private Sector\"\n    NON_PROFIT = \"Non-Profit Organization\"\n\n# Size categories for organizations\nclass EntitySize(str, Enum):\n    SMALL = \"Small (< 50 employees)\"\n    MEDIUM = \"Medium (50-250 employees)\"\n    LARGE = \"Large (> 250 employees)\"\n\n# Compliance areas to assess\nclass ComplianceArea(str, Enum):\n    FINANCIAL_REPORTING = \"Financial Reporting\"\n    INTERNAL_CONTROLS = \"Internal Controls\"\n    ASSET_MANAGEMENT = \"Asset Management\"\n    PROCUREMENT = \"Procurement & Contracts\"\n    HR_PAYROLL = \"HR & Payroll\"\n    IT_SYSTEMS = \"IT Systems & Security\"\n    REGULATORY = \"Regulatory Compliance\"\n    GOVERNANCE = \"Governance & Oversight\"\n\n# Self-assessment status options\nclass AssessmentStatus(str, Enum):\n    COMPLIANT = \"Compliant\"\n    PARTIAL = \"Partially Compliant\"\n    NON_COMPLIANT = \"Non-Compliant\"\n    NOT_ASSESSED = \"Not Yet Assessed\"\n    NOT_APPLICABLE = \"Not Applicable\"\n\n# Risk levels for findings\nclass RiskLevel(str, Enum):\n    CRITICAL = \"Critical\"\n    HIGH = \"High\"\n    MEDIUM = \"Medium\"\n    LOW = \"Low\"\n\n@dataclass\nclass EntityProfile:\n    \"\"\"Organization information for audit assessment.\"\"\"\n    entity_name: str\n    entity_type: str\n    sector: str\n    size_category: str\n    reporting_framework: str  # IFRS, Local GAAP, etc.\n    fiscal_year_end: str\n    years_in_operation: int = 0\n    total_employees: int = 0\n    annual_budget: str = \"\"\n    prior_audit_rating: Optional[str] = None\n    notes: str = \"\"\n\n@dataclass\nclass ComplianceIndicator:\n    \"\"\"Self-assessment for a specific compliance area.\"\"\"\n    area: str\n    status: str  # Compliant, Partial, Non-Compliant, etc.\n    has_documentation: bool = False\n    has_policies: bool = False\n    last_review_date: Optional[str] = None\n    notes: str = \"\"\n\n@dataclass\nclass PriorFinding:\n    \"\"\"Previous audit observation or finding.\"\"\"\n    finding_id: str\n    category: str\n    severity: str  # Critical, High, Medium, Low\n    status: str  # Open, In Progress, Remediated, Recurring\n    description: str\n    year_identified: int = 0\n    remediation_plan: str = \"\"\n    target_date: str = \"\"\n\n@dataclass\nclass IdentifiedGap:\n    \"\"\"A compliance gap identified by the system.\"\"\"\n    gap_id: str\n    area: str\n    description: str\n    risk_level: str\n    requirement_reference: str\n    recommendation: str\n    evidence_needed: List[str] = field(default_factory=list)\n\n@dataclass\nclass RecurringPattern:\n    \"\"\"Pattern detected in recurring findings.\"\"\"\n    pattern_id: str\n    category: str\n    occurrences: int\n    years_recurring: List[int]\n    description: str\n    root_cause_hypothesis: str\n    recommended_action: str\n\n@dataclass\nclass AuditReadinessState:\n    \"\"\"Complete state for audit readiness assessment.\"\"\"\n    entity: Optional[EntityProfile] = None\n    compliance_indicators: List[ComplianceIndicator] = field(default_factory=list)\n    prior_findings: List[PriorFinding] = field(default_factory=list)\n    identified_gaps: List[IdentifiedGap] = field(default_factory=list)\n    recurring_patterns: List[RecurringPattern] = field(default_factory=list)\n    overall_risk_score: float = 0.0\n    readiness_level: str = \"Not Assessed\"\n    assessment_date: str = field(default_factory=lambda: date.today().isoformat())\n\n@dataclass\nclass UnifiedSource:\n    \"\"\"A source document or web reference.\"\"\"\n    provenance: str  # \"local\" or \"web\"\n    source_id: str\n    chunk_id: str\n    content: str\n    score: float\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    def to_citation(self) -> str:\n        return f\"[{self.provenance} | {self.source_id}]\"\n\nprint(\"Data structures defined successfully.\")\nprint(f\"Compliance areas: {[a.value for a in ComplianceArea]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Base Setup\n",
    "\n",
    "The system uses a knowledge base of regulatory documents and standards. This includes:\n",
    "- **IFRS Standards** - International Financial Reporting Standards\n",
    "- **Internal Control Frameworks** - Control requirements and best practices\n",
    "- **ADAA Guidelines** - Abu Dhabi Accountability Authority requirements\n",
    "- **Compliance Checklists** - Standard audit preparation requirements\n",
    "\n",
    "For this demo, we create synthetic documents that represent these standards. In production, you would upload actual regulatory documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File Reading Utilities\n# This cell provides functions to read different file formats\n\ndef read_file_bytes(filename: str, file_bytes: bytes) -> str:\n    \"\"\"Read content from uploaded files (PDF, DOCX, or text).\"\"\"\n    name = filename.lower()\n\n    if name.endswith(\".pdf\"):\n        reader = PdfReader(io.BytesIO(file_bytes))\n        pages = []\n        for i, page in enumerate(reader.pages):\n            txt = page.extract_text() or \"\"\n            pages.append(f\"[PAGE {i+1}] {txt}\")\n        text = \"\\n\".join(pages)\n\n    elif name.endswith(\".docx\"):\n        d = docx.Document(io.BytesIO(file_bytes))\n        text = \"\\n\".join(p.text for p in d.paragraphs)\n\n    else:\n        text = file_bytes.decode(\"utf-8\", errors=\"ignore\")\n\n    # Clean up whitespace\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\nprint(\"File reading utilities ready.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo Standards Documents\n# This cell creates synthetic regulatory documents for demonstration\n\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.units import inch\nimport textwrap\nimport zipfile\n\nout_dir = Path(\"standards_docs\")\nout_dir.mkdir(exist_ok=True)\n\ndef make_pdf(path, title, sections):\n    \"\"\"Create a PDF document with the given title and sections.\"\"\"\n    c = canvas.Canvas(str(path), pagesize=letter)\n    w, h = letter\n    x, y = 0.75*inch, h - 0.9*inch\n\n    c.setFont(\"Helvetica-Bold\", 16)\n    c.drawString(x, y, title)\n    y -= 0.4*inch\n\n    c.setFont(\"Helvetica\", 9)\n    c.drawString(x, y, f\"DEMO DOCUMENT - {date.today()} (For Educational Purposes)\")\n    y -= 0.3*inch\n\n    for header, body in sections:\n        if y < 1.2*inch:\n            c.showPage()\n            y = h - 0.9*inch\n\n        c.setFont(\"Helvetica-Bold\", 12)\n        c.drawString(x, y, header)\n        y -= 0.25*inch\n\n        c.setFont(\"Helvetica\", 11)\n        for line in textwrap.wrap(body, 95):\n            if y < 1.0*inch:\n                c.showPage()\n                y = h - 0.9*inch\n            c.drawString(x, y, line)\n            y -= 0.18*inch\n        y -= 0.15*inch\n\n    c.save()\n\n# --- IFRS Financial Reporting Standards ---\nmake_pdf(\n    out_dir / \"IFRS_Standards_Summary.pdf\",\n    \"IFRS Financial Reporting Standards (Summary)\",\n    [\n        (\"IFRS 1 - First-time Adoption\",\n         \"Entities adopting IFRS for the first time must prepare an opening IFRS statement of \"\n         \"financial position. Full retrospective application is required with limited exemptions. \"\n         \"Comparative information for at least one prior period must be presented.\"),\n        (\"IFRS 15 - Revenue Recognition\",\n         \"Revenue is recognized when control of goods or services transfers to the customer. \"\n         \"The five-step model requires: (1) identify contract, (2) identify performance obligations, \"\n         \"(3) determine transaction price, (4) allocate price, (5) recognize revenue.\"),\n        (\"IFRS 16 - Leases\",\n         \"Lessees must recognize a right-of-use asset and lease liability for most leases. \"\n         \"Short-term leases (under 12 months) and low-value assets may be exempt. \"\n         \"Disclosure of lease obligations and maturity analysis is required.\"),\n        (\"IAS 1 - Presentation of Financial Statements\",\n         \"Financial statements must include: statement of financial position, statement of profit or loss, \"\n         \"statement of changes in equity, statement of cash flows, and notes. \"\n         \"Fair presentation and compliance with IFRS must be explicitly stated.\")\n    ]\n)\n\n# --- Internal Control Framework ---\nmake_pdf(\n    out_dir / \"Internal_Control_Framework.pdf\",\n    \"Internal Control Framework Requirements\",\n    [\n        (\"Control Environment\",\n         \"The organization must establish a control environment that demonstrates commitment to integrity \"\n         \"and ethical values. Board oversight must be independent. Organizational structure must define \"\n         \"clear reporting lines and responsibilities.\"),\n        (\"Risk Assessment\",\n         \"Management must identify and assess risks to achieving objectives. Risk assessment must consider \"\n         \"likelihood and impact. Fraud risk must be explicitly considered. Changes that could affect \"\n         \"internal control must be identified.\"),\n        (\"Control Activities\",\n         \"Control activities must be designed to mitigate identified risks. Segregation of duties is required \"\n         \"for key processes. Authorization controls must be documented. IT general controls must protect \"\n         \"systems and data integrity.\"),\n        (\"Information and Communication\",\n         \"Relevant information must be captured and communicated timely. Internal communication must \"\n         \"support internal control. External communication must be appropriate and controlled.\"),\n        (\"Monitoring Activities\",\n         \"Ongoing monitoring must evaluate control effectiveness. Internal audit function should be \"\n         \"independent. Control deficiencies must be reported to appropriate levels. \"\n         \"Corrective actions must be tracked to completion.\")\n    ]\n)\n\n# --- ADAA Audit Requirements ---\nmake_pdf(\n    out_dir / \"ADAA_Audit_Requirements.pdf\",\n    \"ADAA Audit Requirements and Guidelines\",\n    [\n        (\"Documentation Requirements\",\n         \"Entities must maintain complete and accurate records of all financial transactions. \"\n         \"Supporting documentation must be retained for minimum 7 years. \"\n         \"Electronic records must have appropriate backup and recovery procedures.\"),\n        (\"Financial Reporting Deadlines\",\n         \"Annual financial statements must be prepared within 3 months of fiscal year end. \"\n         \"Quarterly reports are required for government entities. \"\n         \"Audit reports must be submitted within 6 months of year end.\"),\n        (\"Governance Requirements\",\n         \"Audit committee must meet at least quarterly. Internal audit function must report \"\n         \"directly to audit committee. Conflict of interest policies must be documented and enforced. \"\n         \"Whistleblower mechanisms must be established.\"),\n        (\"Asset Management\",\n         \"Fixed asset register must be maintained and reconciled annually. Physical verification \"\n         \"of assets must be performed. Disposal procedures must be documented with proper approvals. \"\n         \"Impairment must be assessed annually.\"),\n        (\"Procurement Standards\",\n         \"Procurement must follow competitive bidding for amounts exceeding thresholds. \"\n         \"Vendor evaluation criteria must be documented. Contract management procedures must be \"\n         \"in place. Purchase orders must precede goods receipt.\")\n    ]\n)\n\n# --- Compliance Checklist ---\nmake_pdf(\n    out_dir / \"Audit_Readiness_Checklist.pdf\",\n    \"Audit Readiness Checklist\",\n    [\n        (\"Financial Reporting Readiness\",\n         \"Checklist: (1) Chart of accounts aligned with reporting framework, (2) Month-end close \"\n         \"procedures documented, (3) Journal entry approval process in place, (4) Reconciliations \"\n         \"performed and reviewed monthly, (5) Financial statement preparation timeline established.\"),\n        (\"Documentation Completeness\",\n         \"Required documents: (1) Board meeting minutes, (2) Policy and procedure manuals, \"\n         \"(3) Organizational charts, (4) Delegation of authority matrix, (5) Risk register, \"\n         \"(6) Internal audit reports, (7) Prior audit reports and management responses.\"),\n        (\"Control Evidence\",\n         \"Evidence to prepare: (1) Bank reconciliations with sign-off, (2) Accounts receivable aging, \"\n         \"(3) Inventory count documentation, (4) Fixed asset verification, (5) Payroll reconciliations, \"\n         \"(6) Access control reviews, (7) IT change management logs.\"),\n        (\"Common Deficiencies\",\n         \"Watch for: (1) Missing segregation of duties, (2) Incomplete supporting documentation, \"\n         \"(3) Untimely reconciliations, (4) Lack of formal policies, (5) Inadequate IT controls, \"\n         \"(6) Unremediated prior findings, (7) Insufficient audit trail.\")\n    ]\n)\n\n# --- HR and Payroll Standards ---\nmake_pdf(\n    out_dir / \"HR_Payroll_Standards.pdf\",\n    \"HR and Payroll Compliance Standards\",\n    [\n        (\"Payroll Processing Controls\",\n         \"Payroll changes must be authorized by HR and approved by department head. \"\n         \"Segregation required between payroll preparation, approval, and payment. \"\n         \"Payroll reconciliation to general ledger must be performed monthly.\"),\n        (\"Employee Records\",\n         \"Personnel files must contain: employment contract, identification documents, \"\n         \"qualifications verification, performance evaluations, and disciplinary records. \"\n         \"Records must be secured with restricted access.\"),\n        (\"Leave and Benefits\",\n         \"Leave balances must be tracked and reconciled. End-of-service benefits must be \"\n         \"calculated according to labor law. Accruals must be recorded in financial statements.\")\n    ]\n)\n\n# --- IT Controls ---\nmake_pdf(\n    out_dir / \"IT_Control_Standards.pdf\",\n    \"IT General Controls Standards\",\n    [\n        (\"Access Controls\",\n         \"User access must follow least-privilege principle. Access reviews must be performed \"\n         \"quarterly. Terminated employee access must be revoked within 24 hours. \"\n         \"Privileged access must be logged and monitored.\"),\n        (\"Change Management\",\n         \"All system changes must be authorized, tested, and approved before implementation. \"\n         \"Segregation required between development and production environments. \"\n         \"Emergency changes must be documented and ratified.\"),\n        (\"Backup and Recovery\",\n         \"Backups must be performed daily for critical systems. Backup restoration must be \"\n         \"tested quarterly. Offsite backup storage is required. \"\n         \"Business continuity plan must be documented and tested annually.\")\n    ]\n)\n\n# Create ZIP of all documents\nzip_path = out_dir / \"standards_docs.zip\"\nwith zipfile.ZipFile(zip_path, \"w\") as z:\n    for f in out_dir.glob(\"*.pdf\"):\n        z.write(f, f.name)\n\nprint(\"Demo standards documents created:\")\nfor f in sorted(out_dir.glob(\"*.pdf\")):\n    print(f\"  - {f.name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Vector Store Setup\n# This cell configures the document indexing and retrieval system\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\n\n# Use a sentence transformer model for embeddings\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Configure text splitting for documents\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=900,\n    chunk_overlap=150,\n    separators=[\"\\n\\n\", \"\\n\", \"Section \", \"Article \", \". \", \" \"]\n)\n\n# Global vector store\nVECTORSTORE = None\n\ndef build_index(files: List[Dict[str, Any]]) -> str:\n    \"\"\"Build the document index from uploaded files.\"\"\"\n    global VECTORSTORE\n\n    docs = []\n    for f in files:\n        text = read_file_bytes(f[\"name\"], f[\"bytes\"])\n        if len(text) < 50:\n            print(f\"Warning: Low text extracted from {f['name']}\")\n        docs.append(Document(page_content=text, metadata={\"source\": f[\"name\"]}))\n\n    chunks = splitter.split_documents(docs)\n    VECTORSTORE = FAISS.from_documents(chunks, embeddings)\n\n    return f\"Indexed {len(files)} document(s) into {len(chunks)} searchable chunks.\"\n\nprint(\"Vector store configured.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LLM Setup\n# This cell configures the language model for analysis\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Main LLM for analysis and report generation\nllm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0.1,\n    api_key=API_KEY,\n    base_url=BASE_URL\n)\n\n# Fast LLM for classification tasks\nllm_fast = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0.0,\n    api_key=API_KEY,\n    base_url=BASE_URL\n)\n\nprint(\"Language model configured.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Integration\n",
    "\n",
    "The system can search the web for additional regulatory guidance and standards. This helps ensure the analysis includes the most current requirements.\n",
    "\n",
    "**Trusted Sources:**\n",
    "- Official government portals (adaa.gov.ae, government.ae)\n",
    "- Standard-setting bodies (ifrs.org, iasb.org)\n",
    "- Professional accounting firms (Big 4)\n",
    "\n",
    "Web search is optional - the system works with local documents alone if Tavily API is not configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Web Search Integration\n# This cell configures web search for additional regulatory sources\n\n# Trusted domains for audit and compliance information\nTRUSTED_DOMAINS = [\n    \"adaa.gov.ae\",        # Abu Dhabi Accountability Authority\n    \"government.ae\",      # UAE Government Portal\n    \"mof.gov.ae\",         # Ministry of Finance\n    \"ifrs.org\",           # IFRS Foundation\n    \"iasb.org\",           # International Accounting Standards Board\n]\n\n# Extended trusted sources (professional commentary)\nEXTENDED_TRUSTED = TRUSTED_DOMAINS + [\n    \"pwc.com\",\n    \"ey.com\",\n    \"kpmg.com\",\n    \"deloitte.com\",\n    \"aicpa.org\",\n]\n\n@dataclass\nclass WebSearchResult:\n    \"\"\"Web search result structure.\"\"\"\n    url: str\n    domain: str\n    title: str\n    snippet: str\n    score: float\n    is_official: bool = False\n\ndef search_web_tavily(query: str, max_results: int = 5) -> List[WebSearchResult]:\n    \"\"\"Search web for compliance and audit information using Tavily.\"\"\"\n    if not TAVILY_API_KEY:\n        return []\n    \n    try:\n        from tavily import TavilyClient\n        client = TavilyClient(api_key=TAVILY_API_KEY)\n        \n        # Enhance query with compliance context\n        enhanced_query = f\"audit compliance {query}\"\n        \n        response = client.search(\n            query=enhanced_query,\n            search_depth=\"advanced\",\n            max_results=max_results * 2,\n            include_answer=False,\n            include_raw_content=False,\n        )\n        \n        results = []\n        for item in response.get(\"results\", []):\n            url = item.get(\"url\", \"\")\n            try:\n                from urllib.parse import urlparse\n                domain = urlparse(url).netloc.replace(\"www.\", \"\")\n            except:\n                domain = url.split(\"/\")[2] if len(url.split(\"/\")) > 2 else url\n            \n            is_official = any(d in domain for d in TRUSTED_DOMAINS)\n            is_trusted = any(d in domain for d in EXTENDED_TRUSTED)\n            \n            # Score adjustment based on source authority\n            base_score = item.get(\"score\", 0.5)\n            if is_official:\n                adjusted_score = min(base_score * 1.4, 1.0)\n            elif is_trusted:\n                adjusted_score = min(base_score * 1.2, 1.0)\n            else:\n                adjusted_score = base_score * 0.8\n            \n            results.append(WebSearchResult(\n                url=url,\n                domain=domain,\n                title=item.get(\"title\", \"\"),\n                snippet=item.get(\"content\", \"\")[:800],\n                score=adjusted_score,\n                is_official=is_official\n            ))\n        \n        results.sort(key=lambda x: (x.is_official, x.score), reverse=True)\n        return results[:max_results]\n        \n    except Exception as e:\n        print(f\"Web search error: {e}\")\n        return []\n\nprint(f\"Web search configured. Tavily enabled: {TAVILY_API_KEY is not None}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Retrieval Functions\n# This cell provides functions to retrieve relevant standards and requirements\n\ndef retrieve_local(query: str, k: int = 6) -> List[UnifiedSource]:\n    \"\"\"Retrieve from local document index.\"\"\"\n    global VECTORSTORE\n    if VECTORSTORE is None:\n        return []\n    \n    results = VECTORSTORE.similarity_search_with_score(query, k=k)\n    sources = []\n    for i, (doc, score) in enumerate(results, start=1):\n        sources.append(UnifiedSource(\n            provenance=\"local\",\n            source_id=doc.metadata.get(\"source\", \"unknown\"),\n            chunk_id=f\"L{i}\",\n            content=doc.page_content[:1200],\n            score=float(score),\n            metadata={\"filename\": doc.metadata.get(\"source\")}\n        ))\n    return sources\n\ndef retrieve_web(query: str, k: int = 3) -> List[UnifiedSource]:\n    \"\"\"Retrieve from web via Tavily.\"\"\"\n    web_results = search_web_tavily(query, max_results=k)\n    sources = []\n    for i, ws in enumerate(web_results, start=1):\n        sources.append(UnifiedSource(\n            provenance=\"web\",\n            source_id=ws.domain,\n            chunk_id=f\"W{i}\",\n            content=ws.snippet,\n            score=ws.score,\n            metadata={\n                \"url\": ws.url,\n                \"title\": ws.title,\n                \"is_official\": ws.is_official\n            }\n        ))\n    return sources\n\ndef retrieve_standards(query: str, enable_web: bool = True) -> List[UnifiedSource]:\n    \"\"\"Retrieve relevant standards from local index and web.\"\"\"\n    all_sources = []\n    \n    # Local retrieval\n    local_sources = retrieve_local(query, k=5)\n    all_sources.extend(local_sources)\n    \n    # Web retrieval (if enabled)\n    if enable_web and TAVILY_API_KEY:\n        web_sources = retrieve_web(query, k=3)\n        all_sources.extend(web_sources)\n    \n    return all_sources\n\ndef format_sources_for_prompt(sources: List[UnifiedSource]) -> str:\n    \"\"\"Format sources for LLM prompt.\"\"\"\n    blocks = []\n    for src in sources:\n        header = src.to_citation()\n        url = src.metadata.get(\"url\", \"\")\n        extra = f\" | URL: {url}\" if url else \"\"\n        blocks.append(f\"SOURCE {src.chunk_id} {header}{extra}:\\n{src.content}\")\n    return \"\\n\\n---\\n\\n\".join(blocks)\n\nprint(\"Retrieval functions ready.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gap Analysis Engine\n",
    "\n",
    "The core of the system is the gap analysis engine. It:\n",
    "\n",
    "1. **Loads relevant standards** based on entity profile\n",
    "2. **Compares compliance indicators** against requirements\n",
    "3. **Analyzes prior findings** for recurring issues\n",
    "4. **Identifies gaps** where compliance is incomplete\n",
    "5. **Scores risk** based on severity and likelihood\n",
    "6. **Generates recommendations** for remediation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Gap Analysis Prompts\n# This cell defines the LLM prompts for gap analysis\n\nGAP_ANALYSIS_PROMPT = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"You are an audit readiness assessment expert. Your task is to identify compliance gaps based on:\n1. The organization's profile and context\n2. Their self-assessment of compliance areas\n3. Any prior audit findings\n4. Relevant regulatory standards and requirements\n\nAnalyze the inputs and identify specific compliance gaps. For each gap:\n- Describe the gap clearly\n- Reference the specific requirement not being met\n- Assess the risk level (Critical, High, Medium, Low)\n- Provide actionable recommendations\n- List evidence that should be prepared\n\nReturn ONLY valid JSON with these keys:\n- gaps: array of gap objects, each with:\n  - gap_id: string (e.g., \"GAP-001\")\n  - area: string (compliance area)\n  - description: string (clear description of the gap)\n  - risk_level: string (Critical, High, Medium, or Low)\n  - requirement_reference: string (which standard/requirement is not met)\n  - recommendation: string (what to do to address it)\n  - evidence_needed: array of strings (documents/evidence to prepare)\n- overall_risk_score: float 0.0-10.0 (10 being highest risk)\n- readiness_level: string (\"Ready\", \"Partially Ready\", \"Not Ready\", \"Critical Gaps\")\n- summary: string (brief overall assessment)\n- priority_actions: array of strings (top 3-5 actions to take)\n\nBe thorough but practical. Focus on material gaps that would concern auditors.\"\"\"),\n    (\"human\", \"\"\"ENTITY PROFILE:\n{entity_profile}\n\nCOMPLIANCE SELF-ASSESSMENT:\n{compliance_indicators}\n\nPRIOR AUDIT FINDINGS:\n{prior_findings}\n\nRELEVANT STANDARDS AND REQUIREMENTS:\n{standards}\n\nAnalyze the above and identify all compliance gaps. Return ONLY JSON.\"\"\")\n])\n\nRECURRING_FINDINGS_PROMPT = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"You are an audit findings analyst. Analyze the prior audit findings to identify recurring patterns.\n\nA recurring pattern is when:\n- Similar issues appear across multiple years\n- The same category has repeated findings\n- Findings are marked as \"Recurring\" status\n- Related root causes are evident\n\nFor each pattern identified, provide:\n- The category affected\n- Number of occurrences\n- Years the issue appeared\n- A hypothesis about the root cause\n- Recommended systemic action to break the cycle\n\nReturn ONLY valid JSON with:\n- patterns: array of pattern objects with:\n  - pattern_id: string (e.g., \"PAT-001\")\n  - category: string\n  - occurrences: number\n  - years_recurring: array of integers\n  - description: string\n  - root_cause_hypothesis: string\n  - recommended_action: string\n- analysis_summary: string (overall assessment of recurring issues)\n- systemic_risk: string (\"High\", \"Medium\", \"Low\") - how entrenched are the issues\"\"\"),\n    (\"human\", \"\"\"PRIOR AUDIT FINDINGS:\n{prior_findings}\n\nCURRENT IDENTIFIED GAPS:\n{current_gaps}\n\nAnalyze for recurring patterns. Return ONLY JSON.\"\"\")\n])\n\nprint(\"Gap analysis prompts defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Gap Analysis Functions\n# This cell implements the gap analysis logic\n\ndef format_entity_profile(entity: EntityProfile) -> str:\n    \"\"\"Format entity profile for prompt.\"\"\"\n    if entity is None:\n        return \"No entity profile provided.\"\n    \n    return f\"\"\"- Entity Name: {entity.entity_name}\n- Entity Type: {entity.entity_type}\n- Sector: {entity.sector}\n- Size: {entity.size_category}\n- Reporting Framework: {entity.reporting_framework}\n- Fiscal Year End: {entity.fiscal_year_end}\n- Years in Operation: {entity.years_in_operation}\n- Total Employees: {entity.total_employees}\n- Annual Budget: {entity.annual_budget}\n- Prior Audit Rating: {entity.prior_audit_rating or 'Not available'}\n- Notes: {entity.notes or 'None'}\"\"\"\n\ndef format_compliance_indicators(indicators: List[ComplianceIndicator]) -> str:\n    \"\"\"Format compliance indicators for prompt.\"\"\"\n    if not indicators:\n        return \"No compliance self-assessment provided.\"\n    \n    lines = []\n    for ind in indicators:\n        lines.append(f\"\"\"- {ind.area}:\n  Status: {ind.status}\n  Documentation: {'Yes' if ind.has_documentation else 'No'}\n  Policies: {'Yes' if ind.has_policies else 'No'}\n  Last Review: {ind.last_review_date or 'Not specified'}\n  Notes: {ind.notes or 'None'}\"\"\")\n    return \"\\n\".join(lines)\n\ndef format_prior_findings(findings: List[PriorFinding]) -> str:\n    \"\"\"Format prior findings for prompt.\"\"\"\n    if not findings:\n        return \"No prior audit findings recorded.\"\n    \n    lines = []\n    for f in findings:\n        lines.append(f\"\"\"- {f.finding_id} ({f.category}):\n  Severity: {f.severity}\n  Status: {f.status}\n  Description: {f.description}\n  Year Identified: {f.year_identified or 'Not specified'}\n  Remediation Plan: {f.remediation_plan or 'None documented'}\"\"\")\n    return \"\\n\".join(lines)\n\ndef _safe_json_parse(text: str) -> Tuple[Dict[str, Any], str]:\n    \"\"\"Safely parse JSON from LLM response.\"\"\"\n    raw = (text or \"\").strip()\n    \n    try:\n        return json.loads(raw), raw\n    except Exception:\n        pass\n    \n    # Try to extract JSON if wrapped in markdown\n    start = raw.find(\"{\")\n    end = raw.rfind(\"}\")\n    if start != -1 and end != -1 and end > start:\n        candidate = raw[start:end+1]\n        try:\n            return json.loads(candidate), raw\n        except Exception:\n            pass\n    \n    return {\"error\": \"Failed to parse LLM response\", \"raw_output\": raw[:3000]}, raw\n\ndef detect_recurring_findings(\n    findings: List[PriorFinding],\n    current_gaps: List[Dict] = None\n) -> Dict[str, Any]:\n    \"\"\"Detect recurring patterns in prior findings.\"\"\"\n    if not findings:\n        return {\"patterns\": [], \"analysis_summary\": \"No prior findings to analyze.\", \"systemic_risk\": \"Low\"}\n    \n    # Format inputs\n    findings_text = format_prior_findings(findings)\n    gaps_text = json.dumps(current_gaps or [], indent=2) if current_gaps else \"None identified yet.\"\n    \n    try:\n        msg = RECURRING_FINDINGS_PROMPT.format_messages(\n            prior_findings=findings_text,\n            current_gaps=gaps_text\n        )\n        response = llm.invoke(msg).content\n        result, _ = _safe_json_parse(response)\n        return result\n    except Exception as e:\n        return {\n            \"patterns\": [],\n            \"analysis_summary\": f\"Analysis failed: {e}\",\n            \"systemic_risk\": \"Unknown\"\n        }\n\ndef analyze_gaps(\n    entity: EntityProfile,\n    indicators: List[ComplianceIndicator],\n    findings: List[PriorFinding],\n    enable_web: bool = True\n) -> Dict[str, Any]:\n    \"\"\"Perform gap analysis and return results.\"\"\"\n    \n    # Build search queries based on entity and indicators\n    search_queries = []\n    if entity:\n        search_queries.append(f\"{entity.reporting_framework} requirements {entity.entity_type}\")\n    \n    for ind in indicators:\n        if ind.status in [\"Partially Compliant\", \"Non-Compliant\", \"Not Yet Assessed\"]:\n            search_queries.append(f\"{ind.area} compliance requirements\")\n    \n    # Retrieve relevant standards\n    all_sources = []\n    for query in search_queries[:5]:  # Limit queries\n        sources = retrieve_standards(query, enable_web=enable_web)\n        all_sources.extend(sources)\n    \n    # Remove duplicates\n    seen = set()\n    unique_sources = []\n    for s in all_sources:\n        key = (s.source_id, s.content[:100])\n        if key not in seen:\n            seen.add(key)\n            unique_sources.append(s)\n    \n    standards_text = format_sources_for_prompt(unique_sources[:10])\n    \n    if not standards_text:\n        standards_text = \"No specific standards retrieved. Use general audit best practices.\"\n    \n    # Run gap analysis\n    try:\n        msg = GAP_ANALYSIS_PROMPT.format_messages(\n            entity_profile=format_entity_profile(entity),\n            compliance_indicators=format_compliance_indicators(indicators),\n            prior_findings=format_prior_findings(findings),\n            standards=standards_text\n        )\n        response = llm.invoke(msg).content\n        result, raw = _safe_json_parse(response)\n        \n        # Detect recurring patterns\n        recurring = detect_recurring_findings(findings, result.get(\"gaps\", []))\n        result[\"recurring_patterns\"] = recurring.get(\"patterns\", [])\n        result[\"recurring_analysis\"] = recurring.get(\"analysis_summary\", \"\")\n        result[\"systemic_risk\"] = recurring.get(\"systemic_risk\", \"Unknown\")\n        \n        # Attach source metadata\n        result[\"_sources\"] = [\n            {\n                \"provenance\": s.provenance,\n                \"source\": s.source_id,\n                \"preview\": s.content[:200],\n                \"url\": s.metadata.get(\"url\", \"\")\n            }\n            for s in unique_sources[:10]\n        ]\n        \n        return result\n        \n    except Exception as e:\n        return {\n            \"error\": f\"Gap analysis failed: {type(e).__name__}: {e}\",\n            \"traceback\": traceback.format_exc()[:2000]\n        }\n\nprint(\"Gap analysis functions ready.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LangGraph Workflow\n# This cell defines the assessment workflow using LangGraph\n\ntry:\n    from langgraph.graph import StateGraph, END\n    from typing import TypedDict\n    LANGGRAPH_AVAILABLE = True\nexcept ImportError:\n    LANGGRAPH_AVAILABLE = False\n    print(\"LangGraph not available, using fallback workflow\")\n\nif LANGGRAPH_AVAILABLE:\n    class WorkflowState(TypedDict):\n        \"\"\"State for the assessment workflow.\"\"\"\n        # Inputs\n        entity: Optional[Dict]\n        indicators: List[Dict]\n        findings: List[Dict]\n        enable_web: bool\n        \n        # Intermediate\n        retrieved_standards: List[Dict]\n        \n        # Outputs\n        gap_analysis: Dict\n        error: Optional[str]\n    \n    def node_retrieve_standards(state: WorkflowState) -> WorkflowState:\n        \"\"\"Retrieve relevant standards based on entity profile.\"\"\"\n        if state.get(\"error\"):\n            return state\n        \n        entity = state.get(\"entity\") or {}\n        indicators = state.get(\"indicators\") or []\n        \n        # Build search queries\n        queries = []\n        if entity.get(\"reporting_framework\"):\n            queries.append(f\"{entity['reporting_framework']} financial reporting requirements\")\n        if entity.get(\"entity_type\"):\n            queries.append(f\"{entity['entity_type']} audit requirements\")\n        \n        for ind in indicators:\n            if ind.get(\"status\") in [\"Partially Compliant\", \"Non-Compliant\"]:\n                queries.append(f\"{ind['area']} compliance standards\")\n        \n        all_sources = []\n        for q in queries[:5]:\n            sources = retrieve_standards(q, enable_web=state.get(\"enable_web\", True))\n            all_sources.extend([asdict(s) for s in sources])\n        \n        state[\"retrieved_standards\"] = all_sources[:15]\n        return state\n    \n    def node_analyze_gaps(state: WorkflowState) -> WorkflowState:\n        \"\"\"Perform gap analysis.\"\"\"\n        if state.get(\"error\"):\n            return state\n        \n        entity = EntityProfile(**state[\"entity\"]) if state.get(\"entity\") else None\n        indicators = [ComplianceIndicator(**i) for i in state.get(\"indicators\", [])]\n        findings = [PriorFinding(**f) for f in state.get(\"findings\", [])]\n        \n        result = analyze_gaps(entity, indicators, findings, enable_web=state.get(\"enable_web\", True))\n        state[\"gap_analysis\"] = result\n        return state\n    \n    # Build workflow graph\n    workflow = StateGraph(WorkflowState)\n    \n    workflow.add_node(\"retrieve_standards\", node_retrieve_standards)\n    workflow.add_node(\"analyze_gaps\", node_analyze_gaps)\n    \n    workflow.set_entry_point(\"retrieve_standards\")\n    workflow.add_edge(\"retrieve_standards\", \"analyze_gaps\")\n    workflow.add_edge(\"analyze_gaps\", END)\n    \n    audit_workflow = workflow.compile()\n    print(\"LangGraph workflow compiled.\")\n\ndef run_assessment(\n    entity: EntityProfile,\n    indicators: List[ComplianceIndicator],\n    findings: List[PriorFinding],\n    enable_web: bool = True\n) -> Dict[str, Any]:\n    \"\"\"Run the complete audit readiness assessment.\"\"\"\n    \n    if LANGGRAPH_AVAILABLE:\n        initial_state = {\n            \"entity\": asdict(entity) if entity else None,\n            \"indicators\": [asdict(i) for i in indicators],\n            \"findings\": [asdict(f) for f in findings],\n            \"enable_web\": enable_web,\n            \"retrieved_standards\": [],\n            \"gap_analysis\": {},\n            \"error\": None\n        }\n        \n        final_state = audit_workflow.invoke(initial_state)\n        return final_state.get(\"gap_analysis\", {\"error\": \"Workflow failed\"})\n    else:\n        # Fallback\n        return analyze_gaps(entity, indicators, findings, enable_web)\n\nprint(\"Assessment workflow ready.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Report Formatting and Evidence Checklist\n# This cell formats the gap analysis results and generates evidence checklists\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport numpy as np\n\ndef generate_evidence_checklist(result: Dict[str, Any], entity: EntityProfile = None) -> str:\n    \"\"\"Generate a downloadable evidence checklist based on identified gaps.\"\"\"\n    \n    gaps = result.get(\"gaps\", [])\n    if not gaps:\n        return \"# Evidence Checklist\\n\\nNo gaps identified - no specific evidence required.\\n\"\n    \n    lines = []\n    lines.append(\"# Audit Evidence Preparation Checklist\")\n    lines.append(\"\")\n    lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n    if entity:\n        lines.append(f\"Entity: {entity.entity_name}\")\n    lines.append(\"\")\n    lines.append(\"---\")\n    lines.append(\"\")\n    lines.append(\"## Instructions\")\n    lines.append(\"Use this checklist to prepare evidence before the audit. Check off each item as you gather the documentation.\")\n    lines.append(\"\")\n    \n    # Group evidence by compliance area\n    evidence_by_area = {}\n    for gap in gaps:\n        area = gap.get(\"area\", \"General\")\n        evidence = gap.get(\"evidence_needed\", [])\n        if area not in evidence_by_area:\n            evidence_by_area[area] = []\n        evidence_by_area[area].extend(evidence)\n    \n    # Remove duplicates within each area\n    for area in evidence_by_area:\n        evidence_by_area[area] = list(set(evidence_by_area[area]))\n    \n    # Generate checklist by area\n    for area, evidence_list in sorted(evidence_by_area.items()):\n        lines.append(f\"## {area}\")\n        lines.append(\"\")\n        for evidence in evidence_list:\n            lines.append(f\"- [ ] {evidence}\")\n        lines.append(\"\")\n    \n    # Add general preparation items\n    lines.append(\"## General Preparation\")\n    lines.append(\"\")\n    lines.append(\"- [ ] Ensure all documents are dated and signed where required\")\n    lines.append(\"- [ ] Verify electronic records have proper access logs\")\n    lines.append(\"- [ ] Prepare organizational chart with current structure\")\n    lines.append(\"- [ ] Compile list of key contacts for each area\")\n    lines.append(\"- [ ] Review and update risk register\")\n    lines.append(\"\")\n    \n    return \"\\n\".join(lines)\n\ndef export_evidence_checklist(result: Dict[str, Any], entity_name: str = \"entity\") -> str:\n    \"\"\"Export evidence checklist as a text file.\"\"\"\n    export_dir = Path(\"exports\")\n    export_dir.mkdir(exist_ok=True)\n    \n    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', entity_name)[:30]\n    file_path = export_dir / f\"evidence_checklist_{safe_name}_{ts}.md\"\n    \n    checklist = generate_evidence_checklist(result, None)\n    \n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(checklist)\n    \n    return str(file_path)\n\ndef generate_compliance_heatmap(indicators: List[ComplianceIndicator], result: Dict[str, Any]) -> str:\n    \"\"\"Generate a compliance heat map visualization.\"\"\"\n    \n    # Define compliance areas and their status\n    all_areas = [\n        \"Financial Reporting\", \"Internal Controls\", \"Asset Management\",\n        \"Procurement & Contracts\", \"HR & Payroll\", \"IT Systems & Security\",\n        \"Regulatory Compliance\", \"Governance & Oversight\"\n    ]\n    \n    # Map status to scores\n    status_scores = {\n        \"Compliant\": 100,\n        \"Partially Compliant\": 60,\n        \"Non-Compliant\": 20,\n        \"Not Yet Assessed\": 40,\n        \"Not Applicable\": None\n    }\n    \n    # Get scores from indicators\n    area_scores = {}\n    for ind in indicators:\n        score = status_scores.get(ind.status, 50)\n        if score is not None:\n            area_scores[ind.area] = score\n    \n    # Adjust scores based on identified gaps\n    gaps = result.get(\"gaps\", [])\n    gap_penalties = {\"Critical\": 30, \"High\": 20, \"Medium\": 10, \"Low\": 5}\n    for gap in gaps:\n        area = gap.get(\"area\", \"\")\n        risk = gap.get(\"risk_level\", \"Medium\")\n        if area in area_scores:\n            area_scores[area] = max(0, area_scores[area] - gap_penalties.get(risk, 10))\n    \n    # Prepare data for visualization\n    areas = []\n    scores = []\n    colors = []\n    \n    for area in all_areas:\n        score = area_scores.get(area, 50)  # Default to 50 if not assessed\n        areas.append(area)\n        scores.append(score)\n        \n        # Color based on score\n        if score >= 80:\n            colors.append('#2ecc71')  # Green\n        elif score >= 60:\n            colors.append('#f1c40f')  # Yellow\n        elif score >= 40:\n            colors.append('#e67e22')  # Orange\n        else:\n            colors.append('#e74c3c')  # Red\n    \n    # Create the figure\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    y_pos = np.arange(len(areas))\n    bars = ax.barh(y_pos, scores, color=colors, edgecolor='white', linewidth=0.5)\n    \n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(areas, fontsize=10)\n    ax.set_xlabel('Compliance Score (%)', fontsize=11)\n    ax.set_title('Compliance Heat Map by Area', fontsize=14, fontweight='bold')\n    ax.set_xlim(0, 100)\n    \n    # Add score labels on bars\n    for bar, score in zip(bars, scores):\n        width = bar.get_width()\n        ax.text(width + 2, bar.get_y() + bar.get_height()/2,\n                f'{int(score)}%', va='center', fontsize=9)\n    \n    # Add legend\n    legend_patches = [\n        mpatches.Patch(color='#2ecc71', label='Good (80-100%)'),\n        mpatches.Patch(color='#f1c40f', label='Moderate (60-79%)'),\n        mpatches.Patch(color='#e67e22', label='Needs Attention (40-59%)'),\n        mpatches.Patch(color='#e74c3c', label='High Risk (<40%)')\n    ]\n    ax.legend(handles=legend_patches, loc='lower right', fontsize=8)\n    \n    ax.invert_yaxis()\n    plt.tight_layout()\n    \n    # Save to file\n    export_dir = Path(\"exports\")\n    export_dir.mkdir(exist_ok=True)\n    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    img_path = export_dir / f\"compliance_heatmap_{ts}.png\"\n    plt.savefig(img_path, dpi=150, bbox_inches='tight', facecolor='white')\n    plt.close()\n    \n    return str(img_path)\n\ndef format_readiness_report(result: Dict[str, Any], entity: EntityProfile = None) -> str:\n    \"\"\"Format gap analysis results into a markdown report.\"\"\"\n    \n    if not isinstance(result, dict):\n        return \"## Error\\n\\nUnexpected result format.\"\n    \n    if \"error\" in result:\n        return f\"\"\"## Error\n\n**{result['error']}**\n\n{result.get('traceback', '')}\"\"\"\n    \n    md = []\n    \n    # Header\n    md.append(\"# Audit Readiness Assessment Report\")\n    md.append(\"\")\n    md.append(f\"**Assessment Date:** {date.today().isoformat()}\")\n    if entity:\n        md.append(f\"**Entity:** {entity.entity_name}\")\n        md.append(f\"**Type:** {entity.entity_type} | **Sector:** {entity.sector}\")\n    md.append(\"\")\n    \n    # Overall Assessment\n    md.append(\"## Overall Assessment\")\n    md.append(\"\")\n    \n    readiness = result.get(\"readiness_level\", \"Not Assessed\")\n    risk_score = result.get(\"overall_risk_score\", 0)\n    \n    # Readiness badge\n    if readiness == \"Ready\":\n        badge = \"LOW RISK - Ready for Audit\"\n    elif readiness == \"Partially Ready\":\n        badge = \"MEDIUM RISK - Some Gaps to Address\"\n    elif readiness == \"Not Ready\":\n        badge = \"HIGH RISK - Significant Gaps\"\n    else:\n        badge = \"CRITICAL RISK - Major Issues\"\n    \n    md.append(f\"**Readiness Level:** {readiness}\")\n    md.append(f\"**Overall Risk Score:** {risk_score:.1f} / 10\")\n    md.append(f\"**Assessment:** {badge}\")\n    md.append(\"\")\n    \n    # Summary\n    if result.get(\"summary\"):\n        md.append(\"### Summary\")\n        md.append(result[\"summary\"])\n        md.append(\"\")\n    \n    # Recurring Patterns Section\n    patterns = result.get(\"recurring_patterns\", [])\n    if patterns:\n        md.append(\"## Recurring Finding Patterns\")\n        md.append(\"\")\n        md.append(f\"**Systemic Risk Level:** {result.get('systemic_risk', 'Unknown')}\")\n        md.append(\"\")\n        if result.get(\"recurring_analysis\"):\n            md.append(f\"_{result['recurring_analysis']}_\")\n            md.append(\"\")\n        \n        for pattern in patterns:\n            md.append(f\"### {pattern.get('pattern_id', 'PAT')} - {pattern.get('category', 'Unknown')}\")\n            md.append(\"\")\n            md.append(f\"- **Occurrences:** {pattern.get('occurrences', 0)} times\")\n            years = pattern.get('years_recurring', [])\n            if years:\n                md.append(f\"- **Years:** {', '.join(map(str, years))}\")\n            md.append(f\"- **Description:** {pattern.get('description', 'N/A')}\")\n            md.append(f\"- **Root Cause Hypothesis:** {pattern.get('root_cause_hypothesis', 'N/A')}\")\n            md.append(f\"- **Recommended Action:** {pattern.get('recommended_action', 'N/A')}\")\n            md.append(\"\")\n    \n    # Priority Actions\n    actions = result.get(\"priority_actions\", [])\n    if actions:\n        md.append(\"## Priority Actions\")\n        md.append(\"\")\n        for i, action in enumerate(actions, 1):\n            md.append(f\"{i}. {action}\")\n        md.append(\"\")\n    \n    # Identified Gaps\n    gaps = result.get(\"gaps\", [])\n    if gaps:\n        md.append(\"## Identified Compliance Gaps\")\n        md.append(\"\")\n        \n        # Group by risk level\n        for risk in [\"Critical\", \"High\", \"Medium\", \"Low\"]:\n            risk_gaps = [g for g in gaps if g.get(\"risk_level\") == risk]\n            if risk_gaps:\n                md.append(f\"### {risk} Risk Gaps\")\n                md.append(\"\")\n                for gap in risk_gaps:\n                    md.append(f\"**{gap.get('gap_id', 'GAP')}** - {gap.get('area', 'General')}\")\n                    md.append(f\"\")\n                    md.append(f\"- **Description:** {gap.get('description', 'N/A')}\")\n                    md.append(f\"- **Requirement:** {gap.get('requirement_reference', 'N/A')}\")\n                    md.append(f\"- **Recommendation:** {gap.get('recommendation', 'N/A')}\")\n                    evidence = gap.get(\"evidence_needed\", [])\n                    if evidence:\n                        md.append(f\"- **Evidence Needed:**\")\n                        for e in evidence:\n                            md.append(f\"  - {e}\")\n                    md.append(\"\")\n    else:\n        md.append(\"## Identified Compliance Gaps\")\n        md.append(\"\")\n        md.append(\"_No specific gaps identified based on the provided information._\")\n        md.append(\"\")\n    \n    # Sources\n    sources = result.get(\"_sources\", [])\n    if sources:\n        md.append(\"## Reference Sources\")\n        md.append(\"\")\n        for src in sources[:5]:\n            url = src.get(\"url\", \"\")\n            if url:\n                md.append(f\"- [{src['source']}]({url})\")\n            else:\n                md.append(f\"- {src['source']}\")\n        md.append(\"\")\n    \n    return \"\\n\".join(md)\n\ndef export_report_pdf(report_md: str, entity_name: str = \"entity\") -> str:\n    \"\"\"Export report as PDF.\"\"\"\n    from reportlab.lib.pagesizes import letter\n    from reportlab.pdfgen import canvas\n    from reportlab.lib.units import inch\n    \n    export_dir = Path(\"exports\")\n    export_dir.mkdir(exist_ok=True)\n    \n    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', entity_name)[:30]\n    pdf_path = export_dir / f\"audit_readiness_{safe_name}_{ts}.pdf\"\n    \n    c = canvas.Canvas(str(pdf_path), pagesize=letter)\n    width, height = letter\n    x = 0.75 * inch\n    y = height - 0.9 * inch\n    \n    c.setFont(\"Helvetica-Bold\", 16)\n    c.drawString(x, y, \"Audit Readiness Assessment Report\")\n    y -= 0.35 * inch\n    \n    c.setFont(\"Helvetica\", 9)\n    c.drawString(x, y, f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    y -= 0.35 * inch\n    \n    c.setFont(\"Helvetica\", 10)\n    \n    for line in report_md.split(\"\\n\"):\n        if y < 0.8 * inch:\n            c.showPage()\n            y = height - 0.9 * inch\n            c.setFont(\"Helvetica\", 10)\n        \n        # Simple formatting\n        line = line.strip()\n        if line.startswith(\"# \"):\n            c.setFont(\"Helvetica-Bold\", 14)\n            line = line[2:]\n        elif line.startswith(\"## \"):\n            c.setFont(\"Helvetica-Bold\", 12)\n            line = line[3:]\n        elif line.startswith(\"### \"):\n            c.setFont(\"Helvetica-Bold\", 11)\n            line = line[4:]\n        else:\n            c.setFont(\"Helvetica\", 10)\n        \n        # Remove markdown formatting\n        line = line.replace(\"**\", \"\").replace(\"_\", \"\")\n        \n        # Wrap long lines\n        for wrapped in textwrap.wrap(line, 90) or [\"\"]:\n            if y < 0.8 * inch:\n                c.showPage()\n                y = height - 0.9 * inch\n            c.drawString(x, y, wrapped)\n            y -= 0.18 * inch\n    \n    c.save()\n    return str(pdf_path)\n\nprint(\"Report formatting and evidence checklist ready.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load Demo Documents\n# This cell loads the demo standards documents into the index\n\nout_dir = Path(\"standards_docs\")\n\ninitial_files = []\nif out_dir.exists():\n    for f_path in out_dir.glob(\"*.pdf\"):\n        with open(f_path, \"rb\") as f:\n            initial_files.append({\"name\": f_path.name, \"bytes\": f.read()})\n\nprint(f\"Found {len(initial_files)} demo standards documents.\")\n\n# Store for UI\nuploaded_files_store = []"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# UI Helper Functions\n# This cell defines helper functions for the user interface\n\ndef ui_upload(files) -> str:\n    \"\"\"Handle file uploads.\"\"\"\n    global uploaded_files_store\n    try:\n        uploaded_files_store = []\n        \n        if not files:\n            return \"No files selected. You can use the demo documents.\"\n        \n        for f in files:\n            if isinstance(f, str) or hasattr(f, \"__fspath__\"):\n                file_path = os.fspath(f)\n                file_name = os.path.basename(file_path)\n                with open(file_path, \"rb\") as fp:\n                    file_bytes = fp.read()\n            elif hasattr(f, \"name\"):\n                file_path = getattr(f, \"name\")\n                file_name = os.path.basename(file_path)\n                with open(file_path, \"rb\") as fp:\n                    file_bytes = fp.read()\n            else:\n                return f\"Unsupported file type: {type(f)}\"\n            \n            uploaded_files_store.append({\"name\": file_name, \"bytes\": file_bytes})\n        \n        return f\"Uploaded {len(uploaded_files_store)} file(s). Click 'Build Index' to process.\"\n    \n    except Exception as e:\n        return f\"Upload failed: {e}\"\n\ndef ui_build_index() -> str:\n    \"\"\"Build the document index.\"\"\"\n    global uploaded_files_store\n    \n    if not uploaded_files_store and initial_files:\n        uploaded_files_store = initial_files\n    \n    if not uploaded_files_store:\n        return \"No documents to index. Please upload files or use demo documents.\"\n    \n    try:\n        return build_index(uploaded_files_store)\n    except Exception as e:\n        return f\"Indexing failed: {e}\"\n\nprint(\"UI helper functions ready.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Interactive Interface\n\nThe interface has five main tabs:\n\n**Tab 1 - Document Setup:** Upload regulatory documents or use the provided demo documents. Build the index to enable retrieval.\n\n**Tab 2 - Organization Profile:** Enter basic organization information including type, sector, size, and reporting framework.\n\n**Tab 3 - Compliance Self-Assessment:** Complete the self-assessment checklist for each compliance area.\n\n**Tab 4 - Prior Audit Findings:** Add any previous audit findings to help identify recurring patterns.\n\n**Tab 5 - Run Assessment:** Generate the gap analysis, view the compliance heat map, and download reports.\n\n### New Features\n\n- **Recurring Findings Detection:** Automatically identifies patterns in prior audit findings and suggests systemic fixes\n- **Compliance Heat Map:** Visual dashboard showing compliance scores by area with color-coded risk levels\n- **Evidence Checklist:** Downloadable checklist of documents to prepare based on identified gaps"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Gradio User Interface\n# This cell creates the interactive web interface\n\nimport gradio as gr\n\n# State storage\ncurrent_entity = None\ncurrent_indicators = []\ncurrent_findings = []\ncurrent_result = {}\n\ndef run_full_assessment(\n    # Entity fields\n    entity_name, entity_type, sector, size, framework, fiscal_year,\n    years_operation, employees, budget, prior_rating, entity_notes,\n    # Compliance indicators (as JSON string from state)\n    indicators_json,\n    # Prior findings (as JSON string from state)\n    findings_json,\n    # Options\n    enable_web\n):\n    \"\"\"Run the complete audit readiness assessment.\"\"\"\n    global current_entity, current_indicators, current_findings, current_result\n    \n    try:\n        # Build entity profile\n        entity = EntityProfile(\n            entity_name=entity_name or \"Unnamed Entity\",\n            entity_type=entity_type,\n            sector=sector,\n            size_category=size,\n            reporting_framework=framework,\n            fiscal_year_end=fiscal_year,\n            years_in_operation=int(years_operation) if years_operation else 0,\n            total_employees=int(employees) if employees else 0,\n            annual_budget=budget or \"\",\n            prior_audit_rating=prior_rating if prior_rating != \"Not Available\" else None,\n            notes=entity_notes or \"\"\n        )\n        current_entity = entity\n        \n        # Parse indicators\n        indicators = []\n        if indicators_json:\n            try:\n                ind_list = json.loads(indicators_json)\n                indicators = [ComplianceIndicator(**i) for i in ind_list]\n            except:\n                pass\n        current_indicators = indicators\n        \n        # Parse findings\n        findings = []\n        if findings_json:\n            try:\n                find_list = json.loads(findings_json)\n                findings = [PriorFinding(**f) for f in find_list]\n            except:\n                pass\n        current_findings = findings\n        \n        # Run assessment\n        result = run_assessment(entity, indicators, findings, enable_web=enable_web)\n        current_result = result\n        \n        # Format outputs\n        report = format_readiness_report(result, entity)\n        \n        # Summary stats\n        gaps = result.get(\"gaps\", [])\n        critical = len([g for g in gaps if g.get(\"risk_level\") == \"Critical\"])\n        high = len([g for g in gaps if g.get(\"risk_level\") == \"High\"])\n        medium = len([g for g in gaps if g.get(\"risk_level\") == \"Medium\"])\n        low = len([g for g in gaps if g.get(\"risk_level\") == \"Low\"])\n        \n        # Recurring patterns summary\n        patterns = result.get(\"recurring_patterns\", [])\n        recurring_text = \"\"\n        if patterns:\n            recurring_text = f\"\\n**Recurring Patterns:** {len(patterns)} detected\"\n            recurring_text += f\"\\n**Systemic Risk:** {result.get('systemic_risk', 'Unknown')}\"\n        \n        summary = f\"\"\"### Assessment Complete\n\n**Readiness Level:** {result.get('readiness_level', 'N/A')}\n**Risk Score:** {result.get('overall_risk_score', 0):.1f} / 10\n\n**Gaps Found:**\n- Critical: {critical}\n- High: {high}\n- Medium: {medium}\n- Low: {low}\n{recurring_text}\n\"\"\"\n        \n        # Generate heat map\n        heatmap_path = None\n        if indicators:\n            try:\n                heatmap_path = generate_compliance_heatmap(indicators, result)\n            except Exception as e:\n                print(f\"Heat map generation failed: {e}\")\n        \n        return (\n            report,\n            summary,\n            json.dumps(result, indent=2, ensure_ascii=False),\n            result,\n            heatmap_path\n        )\n        \n    except Exception as e:\n        error_msg = f\"Assessment failed: {type(e).__name__}: {e}\"\n        return (\n            f\"## Error\\n\\n{error_msg}\",\n            f\"### Error\\n\\n{error_msg}\",\n            json.dumps({\"error\": error_msg}),\n            {},\n            None\n        )\n\ndef add_compliance_indicator(area, status, has_docs, has_policies, last_review, notes, current_json):\n    \"\"\"Add a compliance indicator to the list.\"\"\"\n    try:\n        indicators = json.loads(current_json) if current_json else []\n    except:\n        indicators = []\n    \n    new_indicator = {\n        \"area\": area,\n        \"status\": status,\n        \"has_documentation\": has_docs,\n        \"has_policies\": has_policies,\n        \"last_review_date\": last_review or None,\n        \"notes\": notes or \"\"\n    }\n    indicators.append(new_indicator)\n    \n    # Format for display\n    display = \"\\n\".join([f\"- {i['area']}: {i['status']}\" for i in indicators])\n    \n    return json.dumps(indicators), display\n\ndef clear_indicators():\n    \"\"\"Clear all compliance indicators.\"\"\"\n    return \"\", \"_No indicators added yet_\"\n\ndef add_prior_finding(category, severity, status, description, year, remediation, current_json):\n    \"\"\"Add a prior finding to the list.\"\"\"\n    try:\n        findings = json.loads(current_json) if current_json else []\n    except:\n        findings = []\n    \n    finding_id = f\"F-{len(findings)+1:03d}\"\n    new_finding = {\n        \"finding_id\": finding_id,\n        \"category\": category,\n        \"severity\": severity,\n        \"status\": status,\n        \"description\": description,\n        \"year_identified\": int(year) if year else 0,\n        \"remediation_plan\": remediation or \"\",\n        \"target_date\": \"\"\n    }\n    findings.append(new_finding)\n    \n    # Format for display\n    display = \"\\n\".join([f\"- {f['finding_id']}: {f['category']} ({f['severity']}) - {f['status']}\" for f in findings])\n    \n    return json.dumps(findings), display\n\ndef clear_findings():\n    \"\"\"Clear all prior findings.\"\"\"\n    return \"\", \"_No findings added yet_\"\n\ndef download_report(result_state):\n    \"\"\"Download the report as PDF.\"\"\"\n    global current_entity, current_result\n    \n    if not current_result:\n        raise gr.Error(\"No assessment results. Please run an assessment first.\")\n    \n    report = format_readiness_report(current_result, current_entity)\n    entity_name = current_entity.entity_name if current_entity else \"entity\"\n    return export_report_pdf(report, entity_name)\n\ndef download_checklist(result_state):\n    \"\"\"Download the evidence checklist.\"\"\"\n    global current_entity, current_result\n    \n    if not current_result:\n        raise gr.Error(\"No assessment results. Please run an assessment first.\")\n    \n    entity_name = current_entity.entity_name if current_entity else \"entity\"\n    return export_evidence_checklist(current_result, entity_name)\n\nprint(\"UI functions defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Launch Interface\n# This cell builds and launches the Gradio interface\n\nwith gr.Blocks(title=\"Audit Readiness Assistant\", theme=gr.themes.Soft()) as demo:\n    \n    gr.Markdown(\"\"\"\n# AI-Powered Audit Readiness Assistant\n\nThis system helps assess whether an organization is ready for audit by:\n1. Collecting organization information and compliance self-assessment\n2. Reviewing inputs against regulatory requirements\n3. Identifying compliance gaps and high-risk areas\n4. Detecting recurring finding patterns\n5. Generating an audit readiness report with recommendations\n\n**Instructions:** Complete each tab in order, then run the assessment.\n\"\"\")\n    \n    # Hidden state\n    indicators_state = gr.State(\"\")\n    findings_state = gr.State(\"\")\n    result_state = gr.State({})\n    \n    with gr.Tab(\"1. Document Setup\"):\n        gr.Markdown(\"\"\"\n### Upload Standards Documents\n\nUpload regulatory documents, standards, or guidelines that the assessment should reference.\nDemo documents are provided for testing.\n\"\"\")\n        \n        uploader = gr.File(file_count=\"multiple\", label=\"Upload Documents (PDF, DOCX, TXT)\")\n        upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n        uploader.change(fn=ui_upload, inputs=uploader, outputs=upload_status)\n        \n        build_btn = gr.Button(\"Build Index\", variant=\"primary\")\n        build_status = gr.Textbox(label=\"Index Status\", interactive=False)\n        build_btn.click(fn=ui_build_index, outputs=build_status)\n        \n        gr.Markdown(f\"\"\"\n---\n**Web Search:** {'Enabled' if TAVILY_API_KEY else 'Disabled (add TAVILY_API_KEY to enable)'}\n\n**Demo Documents Available:** {len(initial_files)} standards documents\n\"\"\")\n    \n    with gr.Tab(\"2. Organization Profile\"):\n        gr.Markdown(\"\"\"\n### Enter Organization Details\n\nProvide basic information about the organization being assessed.\n\"\"\")\n        \n        with gr.Row():\n            entity_name = gr.Textbox(label=\"Organization Name\", placeholder=\"e.g., ABC Government Department\")\n            entity_type = gr.Dropdown(\n                label=\"Entity Type\",\n                choices=[\"Government Entity\", \"Semi-Government Entity\", \"Private Sector\", \"Non-Profit Organization\"],\n                value=\"Government Entity\"\n            )\n        \n        with gr.Row():\n            sector = gr.Dropdown(\n                label=\"Sector\",\n                choices=[\"Public Administration\", \"Healthcare\", \"Education\", \"Finance\", \"Infrastructure\", \"Technology\", \"Other\"],\n                value=\"Public Administration\"\n            )\n            size = gr.Dropdown(\n                label=\"Organization Size\",\n                choices=[\"Small (< 50 employees)\", \"Medium (50-250 employees)\", \"Large (> 250 employees)\"],\n                value=\"Medium (50-250 employees)\"\n            )\n        \n        with gr.Row():\n            framework = gr.Dropdown(\n                label=\"Reporting Framework\",\n                choices=[\"IFRS\", \"Local GAAP\", \"IPSAS\", \"US GAAP\", \"Other\"],\n                value=\"IFRS\"\n            )\n            fiscal_year = gr.Textbox(label=\"Fiscal Year End\", placeholder=\"e.g., December 31\", value=\"December 31\")\n        \n        with gr.Row():\n            years_operation = gr.Number(label=\"Years in Operation\", value=10)\n            employees = gr.Number(label=\"Total Employees\", value=150)\n        \n        with gr.Row():\n            budget = gr.Textbox(label=\"Annual Budget\", placeholder=\"e.g., AED 50 million\")\n            prior_rating = gr.Dropdown(\n                label=\"Prior Audit Rating\",\n                choices=[\"Not Available\", \"Unqualified\", \"Qualified\", \"Adverse\", \"Disclaimer\"],\n                value=\"Not Available\"\n            )\n        \n        entity_notes = gr.Textbox(label=\"Additional Notes\", lines=2, placeholder=\"Any relevant context...\")\n    \n    with gr.Tab(\"3. Compliance Self-Assessment\"):\n        gr.Markdown(\"\"\"\n### Compliance Self-Assessment\n\nFor each compliance area, indicate your current status. Add all relevant areas.\n\"\"\")\n        \n        with gr.Row():\n            with gr.Column(scale=2):\n                comp_area = gr.Dropdown(\n                    label=\"Compliance Area\",\n                    choices=[\n                        \"Financial Reporting\", \"Internal Controls\", \"Asset Management\",\n                        \"Procurement & Contracts\", \"HR & Payroll\", \"IT Systems & Security\",\n                        \"Regulatory Compliance\", \"Governance & Oversight\"\n                    ],\n                    value=\"Financial Reporting\"\n                )\n                comp_status = gr.Dropdown(\n                    label=\"Self-Assessment Status\",\n                    choices=[\"Compliant\", \"Partially Compliant\", \"Non-Compliant\", \"Not Yet Assessed\", \"Not Applicable\"],\n                    value=\"Partially Compliant\"\n                )\n                with gr.Row():\n                    has_docs = gr.Checkbox(label=\"Documentation Available\", value=True)\n                    has_policies = gr.Checkbox(label=\"Policies Documented\", value=True)\n                last_review = gr.Textbox(label=\"Last Review Date\", placeholder=\"e.g., 2024-06-30\")\n                comp_notes = gr.Textbox(label=\"Notes\", placeholder=\"Additional context...\")\n                \n                with gr.Row():\n                    add_indicator_btn = gr.Button(\"Add Indicator\", variant=\"primary\")\n                    clear_indicators_btn = gr.Button(\"Clear All\")\n            \n            with gr.Column(scale=1):\n                gr.Markdown(\"### Added Indicators\")\n                indicators_display = gr.Markdown(\"_No indicators added yet_\")\n        \n        add_indicator_btn.click(\n            fn=add_compliance_indicator,\n            inputs=[comp_area, comp_status, has_docs, has_policies, last_review, comp_notes, indicators_state],\n            outputs=[indicators_state, indicators_display]\n        )\n        clear_indicators_btn.click(\n            fn=clear_indicators,\n            outputs=[indicators_state, indicators_display]\n        )\n    \n    with gr.Tab(\"4. Prior Audit Findings\"):\n        gr.Markdown(\"\"\"\n### Prior Audit Findings\n\nEnter any findings from previous audits. This helps identify recurring issues and systemic problems.\n\"\"\")\n        \n        with gr.Row():\n            with gr.Column(scale=2):\n                find_category = gr.Dropdown(\n                    label=\"Category\",\n                    choices=[\n                        \"Financial Reporting\", \"Internal Controls\", \"Asset Management\",\n                        \"Procurement\", \"HR & Payroll\", \"IT Controls\", \"Governance\", \"Other\"\n                    ],\n                    value=\"Internal Controls\"\n                )\n                with gr.Row():\n                    find_severity = gr.Dropdown(\n                        label=\"Severity\",\n                        choices=[\"Critical\", \"High\", \"Medium\", \"Low\"],\n                        value=\"Medium\"\n                    )\n                    find_status = gr.Dropdown(\n                        label=\"Current Status\",\n                        choices=[\"Open\", \"In Progress\", \"Remediated\", \"Recurring\"],\n                        value=\"Open\"\n                    )\n                find_description = gr.Textbox(\n                    label=\"Finding Description\",\n                    lines=2,\n                    placeholder=\"Describe the audit finding...\"\n                )\n                find_year = gr.Number(label=\"Year Identified\", value=2023)\n                find_remediation = gr.Textbox(\n                    label=\"Remediation Plan\",\n                    placeholder=\"What actions have been or will be taken?\"\n                )\n                \n                with gr.Row():\n                    add_finding_btn = gr.Button(\"Add Finding\", variant=\"primary\")\n                    clear_findings_btn = gr.Button(\"Clear All\")\n            \n            with gr.Column(scale=1):\n                gr.Markdown(\"### Added Findings\")\n                findings_display = gr.Markdown(\"_No findings added yet_\")\n        \n        add_finding_btn.click(\n            fn=add_prior_finding,\n            inputs=[find_category, find_severity, find_status, find_description, find_year, find_remediation, findings_state],\n            outputs=[findings_state, findings_display]\n        )\n        clear_findings_btn.click(\n            fn=clear_findings,\n            outputs=[findings_state, findings_display]\n        )\n    \n    with gr.Tab(\"5. Run Assessment\"):\n        gr.Markdown(\"\"\"\n### Generate Audit Readiness Assessment\n\nClick the button below to analyze all inputs and generate the gap analysis report.\n\nThe assessment will:\n- Identify compliance gaps based on your inputs\n- Detect recurring patterns in prior findings\n- Generate a compliance heat map\n- Provide prioritized recommendations\n\"\"\")\n        \n        with gr.Row():\n            enable_web = gr.Checkbox(\n                label=\"Enable Web Search\",\n                value=TAVILY_API_KEY is not None,\n                interactive=TAVILY_API_KEY is not None\n            )\n        \n        run_btn = gr.Button(\"Run Audit Readiness Assessment\", variant=\"primary\", size=\"lg\")\n        \n        gr.Markdown(\"---\")\n        \n        with gr.Row():\n            with gr.Column(scale=1):\n                summary_output = gr.Markdown(\"### Results will appear here\")\n                \n                gr.Markdown(\"### Downloads\")\n                with gr.Row():\n                    download_btn = gr.Button(\"Download Report (PDF)\")\n                    checklist_btn = gr.Button(\"Download Evidence Checklist\")\n                download_file = gr.File(label=\"Download\", interactive=False)\n                \n                gr.Markdown(\"### Compliance Heat Map\")\n                heatmap_output = gr.Image(label=\"Compliance Heat Map\", type=\"filepath\")\n            \n            with gr.Column(scale=2):\n                report_output = gr.Markdown(label=\"Audit Readiness Report\")\n        \n        gr.Markdown(\"---\")\n        \n        with gr.Accordion(\"Raw JSON Output\", open=False):\n            json_output = gr.Code(language=\"json\")\n        \n        run_btn.click(\n            fn=run_full_assessment,\n            inputs=[\n                entity_name, entity_type, sector, size, framework, fiscal_year,\n                years_operation, employees, budget, prior_rating, entity_notes,\n                indicators_state, findings_state, enable_web\n            ],\n            outputs=[report_output, summary_output, json_output, result_state, heatmap_output]\n        )\n        \n        download_btn.click(\n            fn=download_report,\n            inputs=[result_state],\n            outputs=[download_file]\n        )\n        \n        checklist_btn.click(\n            fn=download_checklist,\n            inputs=[result_state],\n            outputs=[download_file]\n        )\n\n# Launch the interface\ndemo.launch(share=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrates an AI-powered audit readiness assistant that:\n\n1. **Accepts organizational inputs** - Entity profile, compliance self-assessment, and prior audit findings\n\n2. **Reviews against requirements** - Uses a knowledge base of standards (IFRS, internal controls, ADAA guidelines) plus optional web search\n\n3. **Identifies compliance gaps** - Compares inputs against requirements to find gaps\n\n4. **Detects recurring patterns** - Analyzes prior findings to identify systemic issues and root causes\n\n5. **Prioritizes by risk** - Scores gaps as Critical, High, Medium, or Low\n\n6. **Generates visual insights** - Creates compliance heat maps showing status by area\n\n7. **Produces actionable outputs** - Provides recommendations, evidence checklists, and PDF reports\n\n### Key Features\n\n| Feature | Description |\n|---------|-------------|\n| **Structured Workflow** | LangGraph-based pipeline for reliable processing |\n| **Hybrid Retrieval** | Combines local documents and web sources |\n| **Recurring Findings Detection** | Identifies patterns across multiple audit years |\n| **Compliance Heat Map** | Visual dashboard of compliance scores by area |\n| **Evidence Checklist** | Downloadable preparation checklist based on gaps |\n| **Risk-Based Prioritization** | Focuses attention on highest-risk areas |\n| **PDF Export** | Formal documentation for audit preparation |\n| **Secure Credentials** | API keys via Colab Secrets |\n\n### Limitations\n\n- Demo documents are synthetic and for illustration only\n- The system supports professional judgment but does not replace it\n- Results depend on quality and completeness of inputs\n\n---\n\n**Project by:** Abdulla Ahmed Alaydaroos\n\n**Purpose:** Capstone Project - AI-Powered Audit Readiness Assistant"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}