{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dkjCawyV8gQ1",
        "EUUA7lxB8v_s",
        "F2N0mWAAK_BJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§¾ **AI-Assisted Tax Audit & Research Copilot**\n",
        "\n",
        "This notebook demonstrates an **AI-assisted tax research and audit support system** designed to help tax auditors and regulatory authorities work faster, more consistently, and with higher confidence.\n",
        "\n",
        "The system allows auditors to:\n",
        "- Ask tax-related questions or describe audit scenarios in natural language\n",
        "- Automatically retrieve relevant tax laws, regulations, and official guidance\n",
        "- Receive a **structured, audit-ready summary** with clear citations to source documents"
      ],
      "metadata": {
        "id": "P97hWzOT7emB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Problem\n",
        "\n",
        "Tax auditors and regulatory authorities must work with a **large and continuously evolving body of tax laws, executive regulations, and official guidance**.\n",
        "\n",
        "Today, this information is:\n",
        "- Spread across multiple documents and formats\n",
        "- Frequently updated through amendments and clarifications\n",
        "- Often searched and summarized manually\n",
        "\n",
        "As a result:\n",
        "- Auditors spend significant time locating relevant provisions\n",
        "- There is a higher risk of missing applicable rules or using outdated guidance\n",
        "- Scaling audit quality across teams becomes difficult\n",
        "\n",
        "This creates a clear need for an **automated assistance system** that supports auditors with fast, reliable, and transparent access to tax information.\n"
      ],
      "metadata": {
        "id": "dkjCawyV8gQ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SqPTVV9Gdxws"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade \\\n",
        "  openai==1.66.3 \\\n",
        "  langchain>=1.0.0 \\\n",
        "  langchain-core>=1.0.0 \\\n",
        "  langchain-openai>=0.3.0 \\\n",
        "  langchain-community>=0.3.0 \\\n",
        "  faiss-cpu \\\n",
        "  sentence-transformers \\\n",
        "  pypdf \\\n",
        "  python-docx \\\n",
        "  gradio \\\n",
        "  pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Secure Model Access\n",
        "\n",
        "The system uses a secured, OpenAI-compatible gateway.  \n",
        "Credentials are loaded from Colab Secrets and are never hard-coded.\n"
      ],
      "metadata": {
        "id": "I5m9ZQ9a8TrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get(\"OPEN_AI_API\")\n",
        "assert API_KEY, \"Missing Colab Secret: OPEN_AI_API\"\n",
        "\n",
        "BASE_URL = \"https://aibe.mygreatlearning.com/openai/v1\"\n",
        "print(\"âœ… Key loaded and gateway set:\", BASE_URL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t76u8YZd6Bx",
        "outputId": "c6329114-c130-4a33-82fd-b91bdf07df74"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Key loaded and gateway set: https://aibe.mygreatlearning.com/openai/v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, io, re, json, traceback\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "from pypdf import PdfReader\n",
        "import docx"
      ],
      "metadata": {
        "id": "Sdh0hrSAgSb_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach Overview\n",
        "\n",
        "The prototype follows a **retrieval-first approach** to audit support.\n",
        "\n",
        "Instead of generating answers from general knowledge, the system:\n",
        "1. Ingests official tax documents provided by the auditor\n",
        "2. Breaks them into searchable sections\n",
        "3. Retrieves only the most relevant excerpts for a given question\n",
        "4. Generates a structured audit summary based strictly on those excerpts\n",
        "\n",
        "Several design decisions were made deliberately:\n",
        "- The system is **source-grounded** and refuses to answer when evidence is missing\n",
        "- All retrieved sources are visible and reviewable\n",
        "- Outputs follow a consistent structure aligned with audit documentation\n",
        "\n",
        "This approach prioritizes **traceability, transparency, and control**."
      ],
      "metadata": {
        "id": "EUUA7lxB8v_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_bytes(filename: str, file_bytes: bytes) -> str:\n",
        "    name = filename.lower()\n",
        "\n",
        "    if name.endswith(\".pdf\"):\n",
        "        reader = PdfReader(io.BytesIO(file_bytes))\n",
        "        pages = []\n",
        "        for i, page in enumerate(reader.pages):\n",
        "            txt = page.extract_text() or \"\"\n",
        "            pages.append(f\"[PAGE {i+1}] {txt}\")\n",
        "        text = \"\\n\".join(pages)\n",
        "\n",
        "    elif name.endswith(\".docx\"):\n",
        "        d = docx.Document(io.BytesIO(file_bytes))\n",
        "        text = \"\\n\".join(p.text for p in d.paragraphs)\n",
        "\n",
        "    else:\n",
        "        text = file_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "XYn3sRVb0z45"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo Documents\n",
        "\n",
        "For demonstration purposes, the notebook generates a small set of **synthetic tax documents** inside the Colab environment.\n",
        "\n",
        "These documents simulate laws, regulations, and official guidance and are used solely to demonstrate the workflow end-to-end.\n",
        "\n",
        "From the systemâ€™s perspective, demo documents are treated exactly the same as real documents uploaded by an auditor.\n"
      ],
      "metadata": {
        "id": "F2N0mWAAK_BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install reportlab"
      ],
      "metadata": {
        "id": "Btn4-WptrAoM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.units import inch\n",
        "from pathlib import Path\n",
        "import textwrap, datetime, zipfile\n",
        "\n",
        "out_dir = Path(\"tax_demo_docs\")\n",
        "out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "def make_pdf(path, title, sections):\n",
        "    c = canvas.Canvas(str(path), pagesize=letter)\n",
        "    w, h = letter\n",
        "    x, y = 0.75*inch, h - 0.9*inch\n",
        "\n",
        "    c.setFont(\"Helvetica-Bold\", 16)\n",
        "    c.drawString(x, y, title)\n",
        "    y -= 0.4*inch\n",
        "\n",
        "    c.setFont(\"Helvetica\", 9)\n",
        "    c.drawString(x, y, f\"SYNTHETIC DEMO DOCUMENT â€” {datetime.date.today()} (NOT REAL LAW)\")\n",
        "    y -= 0.3*inch\n",
        "\n",
        "    for header, body in sections:\n",
        "        if y < 1.2*inch:\n",
        "            c.showPage()\n",
        "            y = h - 0.9*inch\n",
        "\n",
        "        c.setFont(\"Helvetica-Bold\", 12)\n",
        "        c.drawString(x, y, header)\n",
        "        y -= 0.25*inch\n",
        "\n",
        "        c.setFont(\"Helvetica\", 11)\n",
        "        for line in textwrap.wrap(body, 95):\n",
        "            if y < 1.0*inch:\n",
        "                c.showPage()\n",
        "                y = h - 0.9*inch\n",
        "            c.drawString(x, y, line)\n",
        "            y -= 0.18*inch\n",
        "        y -= 0.15*inch\n",
        "\n",
        "    c.save()\n",
        "\n",
        "# --- Synthetic VAT Law ---\n",
        "make_pdf(\n",
        "    out_dir / \"VAT_Law_Demo.pdf\",\n",
        "    \"Synthetic VAT Law (Demo)\",\n",
        "    [\n",
        "        (\"Article 12 â€“ Registration Threshold\",\n",
        "         \"Mandatory VAT registration applies if taxable supplies exceed AED 375,000 \"\n",
        "         \"in the preceding 12 months. Voluntary registration applies from AED 187,500.\"),\n",
        "        (\"Article 22 â€“ Filing Deadline\",\n",
        "         \"VAT returns must be submitted no later than the 28th day following the end \"\n",
        "         \"of the tax period.\"),\n",
        "        (\"Article 59 â€“ Late Filing Penalty\",\n",
        "         \"AED 1,000 for the first late return. AED 2,000 for repeated late returns \"\n",
        "         \"within 24 months.\"),\n",
        "        (\"Article 60 â€“ Late Payment Penalty\",\n",
        "         \"2% immediately after the due date, 4% after 7 days, plus 1% daily thereafter.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- Synthetic Regulation ---\n",
        "make_pdf(\n",
        "    out_dir / \"VAT_Regulation_Demo.pdf\",\n",
        "    \"Synthetic VAT Executive Regulation (Demo)\",\n",
        "    [\n",
        "        (\"Regulation 7 â€“ Small Business Supplies\",\n",
        "         \"Persons below the mandatory registration threshold are not required to \"\n",
        "         \"charge VAT unless voluntarily registered.\"),\n",
        "        (\"Regulation 52 â€“ Penalty Mitigation\",\n",
        "         \"Penalties may be reduced if a justified excuse is accepted by the authority.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- Synthetic Guidance ---\n",
        "make_pdf(\n",
        "    out_dir / \"VAT_Guidance_Demo.pdf\",\n",
        "    \"Synthetic Tax Authority Guidance (Demo)\",\n",
        "    [\n",
        "        (\"GN-07 â€“ Late Filing Review\",\n",
        "         \"Auditors should verify submission timestamps and assigned tax periods.\"),\n",
        "        (\"GN-07 â€“ Audit Checklist\",\n",
        "         \"Check filing history, payment confirmations, turnover evidence, and \"\n",
        "         \"mitigation requests.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ZIP\n",
        "zip_path = out_dir / \"tax_demo_docs.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\") as z:\n",
        "    for f in out_dir.glob(\"*.pdf\"):\n",
        "        z.write(f, f.name)\n",
        "\n",
        "print(\"âœ… Files created in Colab:\")\n",
        "for f in out_dir.iterdir():\n",
        "    print(\" -\", f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4VVAmdMrG5H",
        "outputId": "f1dc5aab-eab2-4888-babd-23148c24fd3d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Files created in Colab:\n",
            " - tax_demo_docs/VAT_Law_Demo.pdf\n",
            " - tax_demo_docs/tax_demo_docs.zip\n",
            " - tax_demo_docs/VAT_Regulation_Demo.pdf\n",
            " - tax_demo_docs/VAT_Guidance_Demo.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audit Workflow\n",
        "\n",
        "The workflow mirrors how auditors typically prepare and analyze source material:\n",
        "\n",
        "1. Relevant documents are uploaded (or loaded from demo data)\n",
        "2. Text is extracted and indexed for retrieval\n",
        "3. The auditor enters a question or audit scenario\n",
        "4. The system retrieves relevant provisions and generates a structured memo\n",
        "5. Source evidence can be inspected directly within the interface\n",
        "\n",
        "This keeps the auditor in control while reducing manual effort.\n"
      ],
      "metadata": {
        "id": "pSkxFDjnLpMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=900,\n",
        "    chunk_overlap=150,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"Article \", \"Section \", \". \", \" \"]\n",
        ")\n",
        "\n",
        "VECTORSTORE = None\n",
        "\n",
        "def build_index(files: List[Dict[str, Any]]) -> str:\n",
        "    global VECTORSTORE\n",
        "\n",
        "    docs = []\n",
        "    for f in files:\n",
        "        text = read_file_bytes(f[\"name\"], f[\"bytes\"])\n",
        "        if len(text) < 50:\n",
        "            # warn but keep going (some docs may have little text)\n",
        "            print(f\"âš ï¸ Low text extracted from {f['name']} (len={len(text)})\")\n",
        "        docs.append(Document(page_content=text, metadata={\"source\": f[\"name\"]}))\n",
        "\n",
        "    chunks = splitter.split_documents(docs)\n",
        "    VECTORSTORE = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    return f\"âœ… Indexed {len(files)} file(s) into {len(chunks)} chunks.\"\n"
      ],
      "metadata": {
        "id": "G0iYl-ZMgVcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ccd6e5c-cd42-4d66-d075-f225221757ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1194074569.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",     # If your gateway requires another model name, change here\n",
        "    temperature=0.1,\n",
        "    api_key=API_KEY,\n",
        "    base_url=BASE_URL\n",
        ")\n",
        "\n",
        "PROMPT = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an AI-assisted tax research and audit support assistant.\n",
        "\n",
        "Rules (STRICT):\n",
        "- Use ONLY the provided SOURCES (snippets). Do not use outside knowledge.\n",
        "- Every factual statement MUST have a citation like [source | chunk_id].\n",
        "- If sources are insufficient, say \"INSUFFICIENT EVIDENCE\" and list what is missing.\n",
        "- Prefer the most authoritative sources if there is conflict.\n",
        "\n",
        "Return ONLY valid JSON with keys:\n",
        "summary (string),\n",
        "key_provisions (array of objects: {{ \"point\": string, \"citation\": string }}),\n",
        "obligations (array of objects: {{ \"item\": string, \"citation\": string }}),\n",
        "exemptions (array of objects: {{ \"item\": string, \"citation\": string }}),\n",
        "penalties (array of objects: {{ \"item\": string, \"citation\": string }}),\n",
        "audit_checklist (array of strings),\n",
        "assumptions (array of strings),\n",
        "sources_used (array of strings).\"\"\"),\n",
        "\n",
        "    (\"human\", \"\"\"AUDIT DATE (as-of): {as_of_date}\n",
        "\n",
        "QUERY / SCENARIO:\n",
        "{query}\n",
        "\n",
        "SOURCES:\n",
        "{sources}\n",
        "\n",
        "Return ONLY JSON. No markdown. No commentary.\"\"\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "xabhlNOpgmLc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _safe_json_parse(text: str) -> Tuple[Dict[str, Any], str]:\n",
        "    raw = (text or \"\").strip()\n",
        "\n",
        "    try:\n",
        "        return json.loads(raw), raw\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Try to extract JSON object if wrapped with extra text\n",
        "    start = raw.find(\"{\")\n",
        "    end = raw.rfind(\"}\")\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "        candidate = raw[start:end+1]\n",
        "        try:\n",
        "            return json.loads(candidate), raw\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return {\"error\": \"Model returned non-JSON or invalid JSON.\", \"raw_output\": raw[:6000]}, raw\n",
        "\n",
        "\n",
        "def retrieve_with_scores(query: str, k: int = 6):\n",
        "    global VECTORSTORE\n",
        "    if VECTORSTORE is None:\n",
        "        return []\n",
        "    results = VECTORSTORE.similarity_search_with_score(query, k=k)\n",
        "    out = []\n",
        "    for i, (doc, score) in enumerate(results, start=1):\n",
        "        doc.metadata[\"chunk_id\"] = f\"r{i}\"\n",
        "        out.append((doc, float(score)))\n",
        "    return out\n",
        "\n",
        "\n",
        "def answer_query(query: str, as_of_date: str = \"Today\", k: int = 6) -> Dict[str, Any]:\n",
        "    global VECTORSTORE\n",
        "    if VECTORSTORE is None:\n",
        "        return {\"error\": \"No index found. Upload documents and click Build Index first.\"}\n",
        "\n",
        "    retrieved = retrieve_with_scores(query, k=k)\n",
        "    if not retrieved:\n",
        "        return {\"error\": \"No retrieved sources. PDFs may have no extractable text.\"}\n",
        "\n",
        "    sources_blocks = []\n",
        "    for i, (d, score) in enumerate(retrieved, start=1):\n",
        "        src = d.metadata.get(\"source\", \"unknown\")\n",
        "        cid = d.metadata.get(\"chunk_id\", f\"r{i}\")\n",
        "        snippet = d.page_content[:1200]\n",
        "        sources_blocks.append(f\"SNIPPET {i} [{src} | {cid} | score={score:.4f}]: {snippet}\")\n",
        "\n",
        "    sources_text = \"\\n\\n\".join(sources_blocks)\n",
        "\n",
        "    try:\n",
        "        msg = PROMPT.format_messages(query=query, as_of_date=as_of_date, sources=sources_text)\n",
        "        out_text = llm.invoke(msg).content\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"LLM call failed: {type(e).__name__}: {e}\"}\n",
        "\n",
        "    parsed, raw = _safe_json_parse(out_text)\n",
        "\n",
        "    # Attach sources for UI transparency\n",
        "    parsed[\"_retrieved_sources\"] = [\n",
        "        {\n",
        "            \"rank\": i,\n",
        "            \"source\": d.metadata.get(\"source\", \"unknown\"),\n",
        "            \"chunk_id\": d.metadata.get(\"chunk_id\", f\"r{i}\"),\n",
        "            \"score\": float(score),\n",
        "            \"preview\": d.page_content[:350].replace(\"\\n\", \" \")\n",
        "        }\n",
        "        for i, (d, score) in enumerate(retrieved, start=1)\n",
        "    ]\n",
        "\n",
        "    return parsed\n"
      ],
      "metadata": {
        "id": "2lmCBwvfiisw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def json_to_memo_md(result: dict) -> str:\n",
        "    if not isinstance(result, dict):\n",
        "        return \"## âŒ Error\\n\\nUnexpected result type.\"\n",
        "\n",
        "    if \"error\" in result:\n",
        "        tb = result.get(\"traceback\", \"\")\n",
        "        raw = result.get(\"raw_output\", \"\")\n",
        "        return (\n",
        "            \"## âŒ Error\\n\\n\"\n",
        "            f\"**{result['error']}**\\n\\n\"\n",
        "            + (f\"### Traceback\\n```text\\n{tb}\\n```\\n\" if tb else \"\")\n",
        "            + (f\"### Raw Output\\n```text\\n{raw}\\n```\\n\" if raw else \"\")\n",
        "        )\n",
        "\n",
        "    def bullets(items, key=\"item\"):\n",
        "        if not items:\n",
        "            return \"_None found in provided sources._\"\n",
        "        out = []\n",
        "        for x in items:\n",
        "            if isinstance(x, dict):\n",
        "                text = x.get(key) or x.get(\"point\") or \"\"\n",
        "                cit = x.get(\"citation\", \"\")\n",
        "                out.append(f\"- {text} **{cit}**\" if cit else f\"- {text}\")\n",
        "            else:\n",
        "                out.append(f\"- {x}\")\n",
        "        return \"\\n\".join(out)\n",
        "\n",
        "    md = []\n",
        "    md.append(\"# ðŸ§¾ Audit Research Memo\")\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Summary\")\n",
        "    md.append(result.get(\"summary\", \"_No summary returned._\"))\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Key Provisions\")\n",
        "    md.append(bullets(result.get(\"key_provisions\", []), key=\"point\"))\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Obligations\")\n",
        "    md.append(bullets(result.get(\"obligations\", []), key=\"item\"))\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Exemptions / Thresholds\")\n",
        "    md.append(bullets(result.get(\"exemptions\", []), key=\"item\"))\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Penalties\")\n",
        "    md.append(bullets(result.get(\"penalties\", []), key=\"item\"))\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Audit Checklist\")\n",
        "    checklist = result.get(\"audit_checklist\", [])\n",
        "    md.append(\"\\n\".join([f\"- {x}\" for x in checklist]) if checklist else \"_None._\")\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Assumptions\")\n",
        "    assumptions = result.get(\"assumptions\", [])\n",
        "    md.append(\"\\n\".join([f\"- {x}\" for x in assumptions]) if assumptions else \"_None._\")\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Sources Used\")\n",
        "    srcs = result.get(\"sources_used\", [])\n",
        "    md.append(\"\\n\".join([f\"- {x}\" for x in srcs]) if srcs else \"_See Retrieved Sources panel._\")\n",
        "\n",
        "    return \"\\n\".join(md)\n"
      ],
      "metadata": {
        "id": "1YO0PqV42p3g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1945d077",
        "outputId": "71cfcb85-3105-43eb-f4af-d7c562f9be10"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "out_dir = Path(\"tax_demo_docs\")  # adjust if your demo docs folder is different\n",
        "\n",
        "initial_uploaded_files_store = []\n",
        "if out_dir.exists():\n",
        "    for f_path in out_dir.glob(\"*.pdf\"):\n",
        "        with open(f_path, \"rb\") as f:\n",
        "            initial_uploaded_files_store.append({\"name\": f_path.name, \"bytes\": f.read()})\n",
        "\n",
        "print(f\"âœ… Demo files found: {len(initial_uploaded_files_store)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Demo files found: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_files_store = []  # populated by ui_upload or fallback to demo files\n",
        "\n",
        "def ui_upload(files) -> str:\n",
        "    global uploaded_files_store\n",
        "    try:\n",
        "        uploaded_files_store = []\n",
        "\n",
        "        if not files:\n",
        "            return \"No new files selected. Use demo files by clicking 'Build Index', or upload new ones.\"\n",
        "\n",
        "        for f in files:\n",
        "            # In Colab, Gradio often returns NamedString (path-like)\n",
        "            if isinstance(f, str) or hasattr(f, \"__fspath__\"):\n",
        "                file_path = os.fspath(f)\n",
        "                file_name = os.path.basename(file_path)\n",
        "                with open(file_path, \"rb\") as fp:\n",
        "                    file_bytes = fp.read()\n",
        "\n",
        "            elif isinstance(f, dict) and \"name\" in f and \"data\" in f:\n",
        "                file_name = f[\"name\"]\n",
        "                file_bytes = f[\"data\"]\n",
        "\n",
        "            elif hasattr(f, \"name\"):\n",
        "                file_path = getattr(f, \"name\")\n",
        "                file_name = os.path.basename(file_path)\n",
        "                with open(file_path, \"rb\") as fp:\n",
        "                    file_bytes = fp.read()\n",
        "\n",
        "            else:\n",
        "                return f\"âŒ Unsupported file object type: {type(f)}\"\n",
        "\n",
        "            uploaded_files_store.append({\"name\": file_name, \"bytes\": file_bytes})\n",
        "\n",
        "        return f\"âœ… Uploaded {len(uploaded_files_store)} file(s). Now click 'Build Index'.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Upload failed: {type(e).__name__}: {e}\\n{traceback.format_exc()}\"\n",
        "\n",
        "\n",
        "def ui_build_index() -> str:\n",
        "    global uploaded_files_store\n",
        "\n",
        "    # fallback to demo docs if no upload\n",
        "    if not uploaded_files_store and initial_uploaded_files_store:\n",
        "        uploaded_files_store = initial_uploaded_files_store\n",
        "\n",
        "    if not uploaded_files_store:\n",
        "        return \"ðŸ›‘ No files to index. Upload PDFs or ensure demo PDFs exist.\"\n",
        "\n",
        "    try:\n",
        "        return build_index(uploaded_files_store)\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Indexing failed: {type(e).__name__}: {e}\\n{traceback.format_exc()}\"\n"
      ],
      "metadata": {
        "id": "EEINTywSilVe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Audit Interface\n",
        "\n",
        "The interface brings the full workflow together in a single view.\n",
        "\n",
        "Auditors can:\n",
        "- Describe an audit scenario in natural language\n",
        "- Review a structured audit memorandum\n",
        "- Inspect the source evidence used\n",
        "- Verify findings before drawing conclusions\n",
        "\n",
        "The interface is intentionally simple and transparent, reflecting how audit tools are used in practice.\n"
      ],
      "metadata": {
        "id": "sWVlC_w2Lug3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import json, traceback\n",
        "import os, re, uuid\n",
        "from datetime import datetime\n",
        "\n",
        "# --- PDF export (memo markdown -> simple PDF) ---\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.units import inch\n",
        "\n",
        "def _memo_md_to_plain_lines(memo_md: str):\n",
        "    \"\"\"\n",
        "    Convert a subset of markdown to plain lines suitable for a PDF.\n",
        "    Keeps headings and bullets readable.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for raw in (memo_md or \"\").splitlines():\n",
        "        s = raw.strip()\n",
        "        if not s:\n",
        "            lines.append(\"\")\n",
        "            continue\n",
        "\n",
        "        # Headings\n",
        "        if s.startswith(\"### \"):\n",
        "            lines.append(s.replace(\"### \", \"\").upper())\n",
        "            continue\n",
        "        if s.startswith(\"## \"):\n",
        "            lines.append(s.replace(\"## \", \"\").upper())\n",
        "            continue\n",
        "        if s.startswith(\"# \"):\n",
        "            lines.append(s.replace(\"# \", \"\").upper())\n",
        "            continue\n",
        "\n",
        "        # Bullets\n",
        "        if s.startswith(\"- \"):\n",
        "            lines.append(\"â€¢ \" + s[2:])\n",
        "            continue\n",
        "\n",
        "        # Remove bold markers\n",
        "        s = s.replace(\"**\", \"\")\n",
        "\n",
        "        lines.append(s)\n",
        "    return lines\n",
        "\n",
        "\n",
        "def export_memo_pdf(memo_md: str, filename_prefix: str = \"audit_memo\") -> str:\n",
        "    \"\"\"\n",
        "    Generates a PDF under a local path and returns the file path for Gradio download.\n",
        "    \"\"\"\n",
        "    export_dir = \"exports\"\n",
        "    os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    uid = uuid.uuid4().hex[:8]\n",
        "    pdf_path = os.path.join(export_dir, f\"{filename_prefix}_{ts}_{uid}.pdf\")\n",
        "\n",
        "    c = canvas.Canvas(pdf_path, pagesize=letter)\n",
        "    width, height = letter\n",
        "\n",
        "    x = 0.75 * inch\n",
        "    y = height - 0.9 * inch\n",
        "\n",
        "    # Title\n",
        "    c.setFont(\"Helvetica-Bold\", 16)\n",
        "    c.drawString(x, y, \"Audit Findings Memorandum\")\n",
        "    y -= 0.35 * inch\n",
        "\n",
        "    c.setFont(\"Helvetica\", 9)\n",
        "    c.drawString(x, y, f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    y -= 0.35 * inch\n",
        "\n",
        "    # Body\n",
        "    c.setFont(\"Helvetica\", 11)\n",
        "    lines = _memo_md_to_plain_lines(memo_md)\n",
        "\n",
        "    # Simple word wrap\n",
        "    def wrap_line(line, max_chars=100):\n",
        "        if len(line) <= max_chars:\n",
        "            return [line]\n",
        "        # basic wrap\n",
        "        chunks = []\n",
        "        words = line.split(\" \")\n",
        "        cur = \"\"\n",
        "        for w in words:\n",
        "            if len(cur) + len(w) + 1 <= max_chars:\n",
        "                cur = (cur + \" \" + w).strip()\n",
        "            else:\n",
        "                chunks.append(cur)\n",
        "                cur = w\n",
        "        if cur:\n",
        "            chunks.append(cur)\n",
        "        return chunks\n",
        "\n",
        "    for line in lines:\n",
        "        if y < 0.8 * inch:\n",
        "            c.showPage()\n",
        "            y = height - 0.9 * inch\n",
        "            c.setFont(\"Helvetica\", 11)\n",
        "\n",
        "        for wl in wrap_line(line, max_chars=100):\n",
        "            if y < 0.8 * inch:\n",
        "                c.showPage()\n",
        "                y = height - 0.9 * inch\n",
        "                c.setFont(\"Helvetica\", 11)\n",
        "\n",
        "            c.drawString(x, y, wl)\n",
        "            y -= 0.18 * inch\n",
        "\n",
        "        if line == \"\":\n",
        "            y -= 0.05 * inch\n",
        "\n",
        "    c.save()\n",
        "    return pdf_path\n",
        "\n",
        "\n",
        "def evidence_badge(df: pd.DataFrame) -> str:\n",
        "    if df is None or df.empty:\n",
        "        return \"### Evidence strength: ðŸ”´ Low\\nNo source evidence retrieved.\"\n",
        "\n",
        "    unique_sources = df[\"source\"].nunique()\n",
        "    total_chunks = len(df)\n",
        "\n",
        "    if unique_sources >= 3:\n",
        "        return (\n",
        "            \"### Evidence strength: ðŸŸ¢ High\\n\"\n",
        "            f\"Evidence drawn from {unique_sources} distinct sources \"\n",
        "            f\"({total_chunks} relevant excerpts).\"\n",
        "        )\n",
        "\n",
        "    if unique_sources == 2:\n",
        "        return (\n",
        "            \"### Evidence strength: ðŸŸ¡ Medium\\n\"\n",
        "            f\"Evidence drawn from {unique_sources} sources \"\n",
        "            f\"({total_chunks} excerpts).\"\n",
        "        )\n",
        "\n",
        "    return (\n",
        "        \"### Evidence strength: ðŸ”´ Low\\n\"\n",
        "        f\"Evidence drawn from a single source ({total_chunks} excerpt(s)).\"\n",
        "    )\n",
        "\n",
        "\n",
        "def ui_ask(query, as_of_date, top_k):\n",
        "    try:\n",
        "        result = answer_query(query=query, as_of_date=as_of_date, k=int(top_k))\n",
        "\n",
        "        sources = result.get(\"_retrieved_sources\", [])\n",
        "        df = pd.DataFrame(sources) if sources else pd.DataFrame(\n",
        "            columns=[\"rank\", \"source\", \"chunk_id\", \"score\", \"preview\"]\n",
        "        )\n",
        "\n",
        "        memo = json_to_memo_md(result)\n",
        "        badge = evidence_badge(df)\n",
        "\n",
        "        # Return JSON + sources table + memo + badge + state result\n",
        "        return json.dumps(result, indent=2, ensure_ascii=False), df, memo, badge, result\n",
        "\n",
        "    except Exception as e:\n",
        "        err = {\n",
        "            \"error\": f\"{type(e).__name__}: {str(e)}\",\n",
        "            \"traceback\": traceback.format_exc()[:6000]\n",
        "        }\n",
        "        df = pd.DataFrame(columns=[\"rank\", \"source\", \"chunk_id\", \"score\", \"preview\"])\n",
        "        memo = json_to_memo_md(err)\n",
        "        badge = \"### Evidence strength: ðŸ”´ Low\\nAn error occurred before evidence could be shown.\"\n",
        "        return json.dumps(err, indent=2, ensure_ascii=False), df, memo, badge, err\n",
        "\n",
        "\n",
        "def show_selected(df: pd.DataFrame, evt: gr.SelectData):\n",
        "    if df is None or df.empty:\n",
        "        return \"No sources to preview.\"\n",
        "    row = df.iloc[evt.index[0]].to_dict()\n",
        "    return (\n",
        "        f\"Source: {row.get('source')}\\n\"\n",
        "        f\"Chunk: {row.get('chunk_id')}\\n\"\n",
        "        f\"Score: {row.get('score')}\\n\\n\"\n",
        "        f\"{row.get('preview')}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def fill_q1():\n",
        "    return \"Late filing VAT return: what are the penalties and what small business thresholds or exemptions apply?\"\n",
        "\n",
        "def fill_q2():\n",
        "    return \"Provide an audit checklist to verify late filing and late payment, including what evidence to request.\"\n",
        "\n",
        "def fill_q3():\n",
        "    return \"What is the corporate income tax rate and what penalties apply?\"\n",
        "\n",
        "\n",
        "def ui_download_pdf(state_result: dict):\n",
        "    \"\"\"\n",
        "    Uses the last result (stored in state) to export the memo as PDF.\n",
        "    \"\"\"\n",
        "    if not isinstance(state_result, dict) or not state_result:\n",
        "        raise gr.Error(\"No memo available yet. Please run a query first.\")\n",
        "\n",
        "    memo_md = json_to_memo_md(state_result)\n",
        "    path = export_memo_pdf(memo_md)\n",
        "    return path\n",
        "\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Tax Audit Copilot\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "# Audit Research & Decision Support Prototype\n",
        "**Workflow:** Upload PDFs â†’ Build Index â†’ Ask scenario â†’ Review memo + evidence â†’ Download memo as PDF.\n",
        "\"\"\")\n",
        "\n",
        "    # Holds last result for download\n",
        "    state_result = gr.State({})\n",
        "\n",
        "    with gr.Tab(\"1) Upload & Index\"):\n",
        "        uploader = gr.File(file_count=\"multiple\", label=\"Upload PDFs/DOCX/TXT\")\n",
        "        upload_status = gr.Textbox(label=\"Upload status\", interactive=False)\n",
        "        uploader.change(fn=ui_upload, inputs=uploader, outputs=upload_status)\n",
        "\n",
        "        build_btn = gr.Button(\"Build Index\", variant=\"primary\")\n",
        "        build_status = gr.Textbox(label=\"Index status\", interactive=False)\n",
        "        build_btn.click(fn=ui_build_index, inputs=None, outputs=build_status)\n",
        "\n",
        "    with gr.Tab(\"2) Ask\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                query = gr.Textbox(\n",
        "                    label=\"Audit question / scenario\",\n",
        "                    lines=4,\n",
        "                    placeholder=\"Example: Late filing VAT return â€” penalties, thresholds, exemptionsâ€¦\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    demo_q1 = gr.Button(\"Demo: Late filing + thresholds\")\n",
        "                    demo_q2 = gr.Button(\"Demo: Audit checklist\")\n",
        "                    demo_q3 = gr.Button(\"Demo: Refusal test\")\n",
        "\n",
        "                as_of = gr.Textbox(label=\"As-of date (audit period)\", value=\"Today\")\n",
        "                topk = gr.Slider(3, 10, value=6, step=1, label=\"Top-K excerpts\")\n",
        "\n",
        "                ask_btn = gr.Button(\"Generate Audit Summary\", variant=\"primary\")\n",
        "\n",
        "                badge_md = gr.Markdown()\n",
        "\n",
        "                download_btn = gr.Button(\"ðŸ“„ Download Audit Memo (PDF)\")\n",
        "                download_file = gr.File(label=\"Download\", interactive=False)\n",
        "\n",
        "            with gr.Column(scale=3):\n",
        "                out_memo = gr.Markdown(label=\"Audit Findings Memorandum\")\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                out_sources = gr.Dataframe(label=\"Source Evidence Used (click a row)\", interactive=False, wrap=True)\n",
        "                selected_preview = gr.Textbox(label=\"Selected Evidence Preview\", lines=8, interactive=False)\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                out_json = gr.Code(label=\"Structured Output (JSON)\", language=\"json\")\n",
        "\n",
        "        # Events\n",
        "        demo_q1.click(fill_q1, outputs=query)\n",
        "        demo_q2.click(fill_q2, outputs=query)\n",
        "        demo_q3.click(fill_q3, outputs=query)\n",
        "\n",
        "        ask_btn.click(\n",
        "            fn=ui_ask,\n",
        "            inputs=[query, as_of, topk],\n",
        "            outputs=[out_json, out_sources, out_memo, badge_md, state_result]\n",
        "        )\n",
        "\n",
        "        out_sources.select(fn=show_selected, inputs=out_sources, outputs=selected_preview)\n",
        "\n",
        "        download_btn.click(fn=ui_download_pdf, inputs=state_result, outputs=download_file)\n",
        "\n",
        "\n",
        "demo.launch(share=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "eHYlUVPa1NQ6",
        "outputId": "28095c44-72b7-4472-f6a7-14c04d6f1497"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2815030389.py:204: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), title=\"Tax Audit Copilot\") as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Reflection\n",
        "\n",
        "This prototype demonstrates how retrieval-based language models can be applied responsibly in audit and regulatory contexts.\n",
        "\n",
        "Key lessons from this implementation include:\n",
        "- AI systems are most effective when used as **assistive tools**, not decision-makers\n",
        "- Transparency and source traceability are essential for trust\n",
        "- Structured outputs aligned with real audit workflows are more valuable than free-form answers\n",
        "\n",
        "With appropriate governance, the same approach could be extended to larger document sets, additional tax domains, or integrated audit platforms.\n",
        "\n",
        "Ultimately, the value of the system lies not in automation alone, but in **supporting better, faster, and more consistent professional judgment**.\n",
        "\n",
        "**Done by: Abdulla Ahmed Alaydaroos**\n"
      ],
      "metadata": {
        "id": "HRLsI6LcLw7f"
      }
    }
  ]
}